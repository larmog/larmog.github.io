<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arm on kodbasen</title>
    <link>http://larmog.github.io/categories/arm/</link>
    <description>Recent content in Arm on kodbasen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Â© 2016 by larmog</copyright>
    <lastBuildDate>Fri, 28 Oct 2016 05:15:54 +0200</lastBuildDate>
    <atom:link href="http://larmog.github.io/categories/arm/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Installing Kubernetes on ARM with kubeadm</title>
      <link>http://larmog.github.io/2016/10/28/installing-kubernetes-on-arm-with-kubeadm/</link>
      <pubDate>Fri, 28 Oct 2016 05:15:54 +0200</pubDate>
      
      <guid>http://larmog.github.io/2016/10/28/installing-kubernetes-on-arm-with-kubeadm/</guid>
      <description>

&lt;p&gt;The tool &lt;code&gt;kubeadm&lt;/code&gt; is an awesome tool for getting started with Kubernetes.
This short post shows you how to get started on &lt;code&gt;Raspberry Pi 3&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&#34;os&#34;&gt;OS&lt;/h5&gt;

&lt;p&gt;I&amp;rsquo;ve used HypriotOS &lt;a href=&#34;http://blog.hypriot.com/post/releasing-HypriotOS-1-0/&#34;&gt;&amp;ldquo;Blackbeard&amp;rdquo;&lt;/a&gt;
when using &lt;code&gt;Raspberry PI 3&lt;/code&gt; and &lt;a href=&#34;http://oph.mdrjr.net/odrobian/doc/getting-started.html&#34;&gt;Odrobian&lt;/a&gt; for &lt;code&gt;Odroid C2&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For some reason &lt;code&gt;cgroup=cpuset&lt;/code&gt; wasn&amp;rsquo;t enabled in the kernel for HypriotOS so
I had to enable it in &lt;code&gt;/boot/cmdline.txt&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;HypriotOS/armv7: root@minion1 in ~
$ cat /boot/cmdline.txt
+dwc_otg.lpm_enable=0 console=tty1 root=/dev/mmcblk0p2 rootfstype=ext4 cgroup_enable=cpuset cgroup_enable=memory swapaccount=1 elevator=deadline fsck.repair=yes rootwait console=ttyAMA0,115200 kgdboc=ttyAMA0,115200
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;installing&#34;&gt;Installing&lt;/h5&gt;

&lt;p&gt;The guide &lt;a href=&#34;http://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;Installing Kubernetes on Linux with kubeadm&lt;/a&gt; shows
you how to install, if you scroll down to the end of the page (which I didn&amp;rsquo;t
and wish I had) you can see how to install Kubernetes using &lt;code&gt;kubeadm&lt;/code&gt; for &lt;code&gt;ARM&lt;/code&gt;.
Thank&amp;rsquo;s to &lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas&lt;/a&gt; I guess.&lt;/p&gt;

&lt;p&gt;Also while reading the official documentation
for &lt;a href=&#34;http://kubernetes.io/docs/admin/kubeadm/&#34;&gt;kubeadm&lt;/a&gt;, there&amp;rsquo;s
a &lt;code&gt;kubeadm init&lt;/code&gt; flag &lt;code&gt;--use-kubernetes-version&lt;/code&gt; you should set to
at least &lt;code&gt;v1.4.3&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&#34;using-weave-kube&#34;&gt;Using &lt;code&gt;weave-kube&lt;/code&gt;&lt;/h5&gt;

&lt;p&gt;I didn&amp;rsquo;t read the last part of the guide (or it wasn&amp;rsquo;t present while I read it)
so I had to do things the hard way (or backwards). The good thing is that
you can use my &lt;code&gt;ARM&lt;/code&gt; version of &lt;a href=&#34;https://github.com/weaveworks/weave-kube&#34;&gt;weave-kube&lt;/a&gt;
instead of flannel, for pod networking.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl create -f https://raw.githubusercontent.com/kodbasen/weave-kube-arm/master/weave-daemonset.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h5&gt;

&lt;p&gt;&lt;code&gt;kubeadm&lt;/code&gt; is awesome :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building your Kubernetes cluster in minutes</title>
      <link>http://larmog.github.io/2016/07/14/building-your-kubernetes-cluster-in-minutes/</link>
      <pubDate>Thu, 14 Jul 2016 19:16:43 +0200</pubDate>
      
      <guid>http://larmog.github.io/2016/07/14/building-your-kubernetes-cluster-in-minutes/</guid>
      <description>&lt;p&gt;With the release of Kubernetes &lt;code&gt;v1.3.0&lt;/code&gt; there is now a cross platform way of
quickly setting up a Kubernetes cluster using Docker.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kube-deploy/tree/master/docker-multinode&#34;&gt;kube-deploy/docker-multinode&lt;/a&gt;
uses two instances of Docker daemon.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve created a hybrid solution &lt;a href=&#34;https://github.com/kodbasen/sloop&#34;&gt;Sloop&lt;/a&gt; that uses kube-deploy/docker-multinode but runs
the kubelet &lt;em&gt;native&lt;/em&gt;. The reason for running your &lt;code&gt;kubelet&lt;/code&gt;:s &lt;em&gt;native&lt;/em&gt; is the full support of &lt;a href=&#34;http://kubernetes.io/docs/user-guide/persistent-volumes/&#34;&gt;Persistent Volumes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Sloop uses the best of two worlds. Docker for bootstrapping your cluster and &lt;em&gt;native&lt;/em&gt; kubelet for full Kubernetes functionality.&lt;/p&gt;

&lt;p&gt;Give it a try and see what you think!
&lt;a href=&#34;https://github.com/kodbasen/sloop&#34;&gt;https://github.com/kodbasen/sloop&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>E(F)K cluster on Kubernetes-On-ARM - Part 2</title>
      <link>http://larmog.github.io/2016/05/02/efk-cluster-on-kubernetes-on-arm---part-2/</link>
      <pubDate>Mon, 02 May 2016 08:55:37 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/05/02/efk-cluster-on-kubernetes-on-arm---part-2/</guid>
      <description>

&lt;p&gt;This is the second part in a series about handling logs on Kubernetes-On-ARM.
In the first part we installed ELK and started sending syslog events from our
nodes using &lt;code&gt;logstash-forwarder&lt;/code&gt;. In this part we will start collecting logs
from our &lt;code&gt;pods&lt;/code&gt; and Kubernetes components. If you wan&amp;rsquo;t to cache up here&amp;rsquo;s a
list of previous posts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;Kubernetes-On-ARM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/03/13/elk-cluster-on-kubernetes-on-arm---part-1/&#34;&gt;ELK cluster on Kubernetes-On-ARM - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The plan was that this part would be about how
to start collecting logs from Kubernetes. But I wasn&amp;rsquo;t satisfied with how
&lt;code&gt;logstash-forwarder&lt;/code&gt; worked. The thing is that, once the &lt;code&gt;logstash-forwarder&lt;/code&gt;
daemon is started, the node can&amp;rsquo;t run much else.&lt;/p&gt;

&lt;p&gt;Another problem with the first attempt was that we were collecting &lt;code&gt;syslog&lt;/code&gt;
messages instead of collecting logs directly from &lt;code&gt;journald&lt;/code&gt;. There is no
&lt;code&gt;syslog&lt;/code&gt; on &lt;code&gt;Arch Linux&lt;/code&gt; by default. You need to install &lt;code&gt;syslog-ng&lt;/code&gt; in order to
make it available. That doesn&amp;rsquo;t make sense when running on a &lt;code&gt;Raspberry-Pi&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A better solution is to collect the logs directly from the source, and send them
to &lt;code&gt;elasticsearch&lt;/code&gt;. In this second part we will replace &lt;code&gt;logstash-forwarder&lt;/code&gt; and
&lt;code&gt;logstash&lt;/code&gt; with &lt;code&gt;fluentd&lt;/code&gt; for collecting logs. We will add two &lt;code&gt;systemd&lt;/code&gt;
services on each node that will keep track of the position in the &lt;code&gt;journald&lt;/code&gt; and
send logs as &lt;code&gt;JSON&lt;/code&gt; to &lt;code&gt;fluentd&lt;/code&gt; using &lt;code&gt;ncat&lt;/code&gt;. Fluentd will receive the logs
from our service and also collect logs from &lt;code&gt;docker&lt;/code&gt; containers on each host
and add metadata from &lt;code&gt;Kubernetes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s get started :)&lt;/p&gt;

&lt;p&gt;If you haven&amp;rsquo;t upgraded to &lt;code&gt;Kubernetes-On-ARM v0.7.0&lt;/code&gt; and &lt;code&gt;Kubernetes 1.2&lt;/code&gt;, do
that first.&lt;/p&gt;

&lt;h4 id=&#34;upgrading-to-kibana-4-5-0-and-elasticsearch-2-3-2&#34;&gt;Upgrading to Kibana 4.5.0 and Elasticsearch 2.3.2&lt;/h4&gt;

&lt;p&gt;I have created images for &lt;code&gt;elasticsearch&lt;/code&gt; and &lt;code&gt;kibana&lt;/code&gt; that can be found here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/kodbasen/elasticsearch-kubernetes-armhf/&#34;&gt;https://hub.docker.com/r/kodbasen/elasticsearch-kubernetes-armhf/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/kodbasen/kibana-armhf/&#34;&gt;https://hub.docker.com/r/kodbasen/kibana-armhf/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to upgrade &lt;code&gt;elasticsearch&lt;/code&gt; to version &lt;code&gt;2.3.2&lt;/code&gt; and &lt;code&gt;kibana&lt;/code&gt; version
&lt;code&gt;4.5.0&lt;/code&gt; run the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Warning this will remove your elasticsearch from Part 1
$ kubectl delete -f https://raw.githubusercontent.com/kodbasen/elasticsearch-kubernetes-armhf/master/elasticsearch.yaml
$ kubectl create -f https://raw.githubusercontent.com/kodbasen/elasticsearch-kubernetes-armhf/master/elasticsearch.yaml
$ # Warning this will delete your kibana from Part 1
$ kubectl delete -f https://raw.githubusercontent.com/kodbasen/kibana-armhf/master/kibana.yaml
$ kubectl create -f https://raw.githubusercontent.com/kodbasen/kibana-armhf/master/kibana.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;collecting-journald-logs&#34;&gt;Collecting journald logs&lt;/h4&gt;

&lt;p&gt;I found this solution from &lt;a href=&#34;https://github.com/ianblenke/docker-fluentd/tree/master/systemd&#34;&gt;ianblenke&lt;/a&gt;
and I have created a repo here: &lt;a href=&#34;https://github.com/kodbasen/fluentd-kubernetes&#34;&gt;fluentd-kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how to install the services on each node in your cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Install ncat first (apt-get install nmap or pacman -S nmap)
$ # Clone the repo
$ git clone https://github.com/kodbasen/fluentd-kubernetes &amp;amp;&amp;amp; cd fluentd-kubernetes
$ sudo cp systemd/*.service /usr/lib/systemd/system
$ sudo systemctl enable journald-fluentd-pos.service
$ sudo systemctl restart journald-fluentd-pos.service
$ sudo systemctl enable journald-fluentd.service
$ sudo systemctl restart journald-fluentd.service
$ # You can verify that the streaming works by using the following cmd:
$ ncat -l -k 5170
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;journald-fluentd-pos.service&lt;/code&gt; will keep track of the &lt;code&gt;journald&lt;/code&gt; position
and &lt;code&gt;journald-fluentd.service&lt;/code&gt; will follow &lt;code&gt;journald&lt;/code&gt; in &lt;code&gt;JSON&lt;/code&gt; format and send
the logs to &lt;code&gt;fluentd&lt;/code&gt; on port &lt;code&gt;5170&lt;/code&gt; using &lt;code&gt;ncat&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The next step is to start our &lt;code&gt;daemonSet&lt;/code&gt;, that will collect all logs on the node
and send them to &lt;code&gt;elasticsearch&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/kodbasen/fluentd-kubernetes &amp;amp;&amp;amp; cd fluentd-kubernetes
$ kubectl create -f fluentd-ds.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now see how your &lt;code&gt;fluentd&lt;/code&gt; daemons are starting on all your nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods
NAME                      READY     STATUS    RESTARTS   AGE
es-client-nrlbv           1/1       Running   0          2h
es-client-tm618           1/1       Running   0          2h
es-data-bvsqx             1/1       Running   0          2h
es-master-skdmf           1/1       Running   0          2h
fluentd-319fq             1/1       Running   0          16h
fluentd-41sml             1/1       Running   0          16h
fluentd-5kzt6             1/1       Running   0          16h
fluentd-86scv             1/1       Running   0          16h
fluentd-gqxr8             1/1       Running   0          16h
fluentd-rkod8             1/1       Running   0          16h
fluentd-u0vhl             1/1       Running   0          16h
fluentd-uqx3g             1/1       Running   0          16h
fluentd-zs06d             1/1       Running   0          16h
kibana-6rqgk              1/1       Running   3          2d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our &lt;code&gt;fluentd&lt;/code&gt; daemon accepts incoming logs on &lt;code&gt;tcp&lt;/code&gt; port &lt;code&gt;5170&lt;/code&gt; and tags them
with &lt;code&gt;systemd&lt;/code&gt; it will also tail all logs found in &lt;code&gt;/var/log/containers&lt;/code&gt; and
tag them with &lt;code&gt;kubelet.*&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Open Kibana in your browser and you should see that your logs are beeing stored
in &lt;code&gt;elasticsearch&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve created two saved searches, one foreach tag:&lt;/p&gt;

&lt;p&gt;
&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Systemd tag search in Kibana&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/systemd-tag.png&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;



&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Kubelet tag search in Kibana&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/kubelet-tag.png&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;h4 id=&#34;to-sum-up&#34;&gt;To sum up&lt;/h4&gt;

&lt;p&gt;Now we can collect logs from both &lt;code&gt;PODS&lt;/code&gt; and &lt;code&gt;nodes&lt;/code&gt;. But you will soon discover
that our nodes generates a huge amount of logs. Basically were running the same
software as on the &lt;code&gt;x86_64&lt;/code&gt; platform but with much less resources
(memory, cpu and disk). The old saying &lt;strong&gt;Silence is golden&lt;/strong&gt; is very true
when it comes to logging. Our applications and components should only report
when something is wrong, and even then, the log should be a single line carrying
just enough information to identify the problem. If you take a look at the
stacktraces from &lt;code&gt;elasticsearch&lt;/code&gt; and &lt;code&gt;Java&lt;/code&gt; that is written to &lt;code&gt;stderr&lt;/code&gt;, each
line generates a log message. A single error, written to &lt;code&gt;stderr&lt;/code&gt;, with a
stacktrace can result in a huge amount of log messages. I leave it up to you to
filter out unnecessary logs and aggregate and combine multiple rows in to one
log message. All to minimize the noise.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ELK cluster on Kubernetes-On-ARM - Part 1</title>
      <link>http://larmog.github.io/2016/03/13/elk-cluster-on-kubernetes-on-arm---part-1/</link>
      <pubDate>Sun, 13 Mar 2016 22:44:21 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/03/13/elk-cluster-on-kubernetes-on-arm---part-1/</guid>
      <description>

&lt;p&gt;One of the most important parts of running a cluster is to gain knowledge of
whats going on. Using tools like &lt;code&gt;kubectl logs&lt;/code&gt; or &lt;code&gt;docker logs&lt;/code&gt; is fine if you
run one or two nodes, but it soon gets impossible to get an overview of whats
going on, and you need to be able to view, query and monitor your logs from one
central place.&lt;/p&gt;

&lt;p&gt;This blog post is about setting up
&lt;em&gt;Elasticsearch + Logstash + Kibana ELK on Kubernetes-On-ARM&lt;/em&gt;. You could question
the use of running &lt;em&gt;ELK&lt;/em&gt; on an &lt;em&gt;ARM&lt;/em&gt; cluster of &lt;em&gt;Raspberry Pi&lt;/em&gt;:s, and you are
right, it is a bit to heavy. But I use the cluster as a way of learning, and the
steps are the same as if you are running on &lt;code&gt;x86_64&lt;/code&gt; or in the cloud. Thereâs also
a bunch of new &lt;code&gt;ARM64&lt;/code&gt; single-board computers on the way, &lt;em&gt;Raspberry Pi 3&lt;/em&gt;
being the first.&lt;/p&gt;

&lt;p&gt;You can use &lt;a href=&#34;http://www.fluentd.org/&#34;&gt;&lt;code&gt;fluentd&lt;/code&gt;&lt;/a&gt; instead of &lt;code&gt;logstash&lt;/code&gt; but we
stick to &lt;code&gt;logstash&lt;/code&gt; for now.&lt;/p&gt;

&lt;p&gt;Setting up &lt;em&gt;ELK&lt;/em&gt; on Kubernetes is nothing new, I&amp;rsquo;m using Paulo Pires:s
&lt;a href=&#34;https://github.com/larmog/kubernetes-elasticsearch-cluster&#34;&gt;kubernetes-elasticsearch-cluster&lt;/a&gt;
and &lt;a href=&#34;https://github.com/larmog/kubernetes-elk-cluster&#34;&gt;kubernetes-elk-cluster&lt;/a&gt;,
modified for ARM.&lt;/p&gt;

&lt;p&gt;The thing is, how do we collect our container logs and send them to logstash?
The first alternative is to use
&lt;a href=&#34;https://github.com/elastic/logstash-forwarder&#34;&gt;logstash-forwarder&lt;/a&gt; but the
project has been replaced by
&lt;a href=&#34;https://github.com/elastic/beats/tree/master/filebeat&#34;&gt;filebeat&lt;/a&gt;, so that
becomes our second alternative. Then I found &lt;em&gt;gliderlabs&lt;/em&gt;
&lt;a href=&#34;https://github.com/gliderlabs/logspout&#34;&gt;logspout&lt;/a&gt; project and &lt;em&gt;looplab&lt;/em&gt;:s
&lt;a href=&#34;https://github.com/looplab/logspout-logstash&#34;&gt;logspout-logstash&lt;/a&gt; module, which
is a third alternative.&lt;/p&gt;

&lt;p&gt;If you want to use &lt;code&gt;kubectl logs&lt;/code&gt; or &lt;code&gt;docker logs&lt;/code&gt; you
need to stick to the &lt;code&gt;json-file&lt;/code&gt; or &lt;code&gt;journald&lt;/code&gt; logging driver in Docker. The
&lt;code&gt;json-file&lt;/code&gt; driver is rather resource consuming and not a great alternative for
production. There&amp;rsquo;s a logstash plugin for
&lt;a href=&#34;https://github.com/logstash-plugins/logstash-input-journald&#34;&gt;&lt;code&gt;journald&lt;/code&gt;&lt;/a&gt; that
looks promising and I hope to explore the different alternatives in the up
coming posts. Using &lt;code&gt;journald&lt;/code&gt; for Docker means that we can use the same
mechanism for sending logs for hosts and containers.&lt;/p&gt;

&lt;p&gt;To summarize what we want to achieve:
&lt;div class=&#34;card-panel teal&#34;&gt;&lt;span class=&#34;white-text&#34;&gt;
&lt;em&gt;We want to collect the logs and collect
them as quick as possible, but still be able to use the tools we&amp;rsquo;re used to like
&lt;code&gt;docker logs&lt;/code&gt; and &lt;code&gt;kubectl logs&lt;/code&gt;.&lt;/em&gt;
&lt;/span&gt;&lt;/div&gt;
We&amp;rsquo;ll see how much of this we can achieve.&lt;/p&gt;

&lt;p&gt;But we need to start somewhere, so let&amp;rsquo;s get started with &lt;code&gt;logstash-forwarder&lt;/code&gt;
and collect &lt;code&gt;syslog&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;elasticsearch&#34;&gt;Elasticsearch&lt;/h4&gt;

&lt;p&gt;First we need to get &lt;code&gt;elasticsearch&lt;/code&gt; up and running. I have prepared all
necessary docker images so all you need to do is to deploy them to your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Clone the git repo
$ git clone https://github.com/larmog/kubernetes-elasticsearch-cluster.git
$ cd kubernetes-elasticsearch-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Follow Paulo&amp;rsquo;s instructions to &lt;a href=&#34;https://github.com/larmog/kubernetes-elasticsearch-cluster#deploy&#34;&gt;deploy&lt;/a&gt;
the cluster. I recommend that you wait until each component is provisioned
before you deploy the next.&lt;/p&gt;

&lt;p&gt;If all went well, you should now have &lt;code&gt;elasticsearch&lt;/code&gt; running. You can check the
status using this command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run --rm --dns=10.0.0.10 hypriot/armhf-busybox wget -q -O -  http://elasticsearch:9200/_cluster/health?pretty
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;logstash-kibana&#34;&gt;Logstash + Kibana&lt;/h4&gt;

&lt;p&gt;Pauloâs solution uses the Lumberjack secure protocol, and you need generate your
own certificate and key for &lt;code&gt;logstash.default.svc.cluster.local&lt;/code&gt;. I am using
&lt;code&gt;easyrsa3&lt;/code&gt;. Make sure that you have valid &lt;code&gt;.key&lt;/code&gt; and &lt;code&gt;.crt&lt;/code&gt; files. You can of
course change your protocol to something other than &lt;code&gt;lumberjack&lt;/code&gt; and make the
corresponding change in &lt;code&gt;logstash-forwarder&lt;/code&gt;. I will show you how to setup
&lt;code&gt;lumberjack&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Install Kelsey Hightower:s tool
&lt;a href=&#34;https://github.com/kelseyhightower/conf2kube&#34;&gt;&lt;code&gt;conf2kube&lt;/code&gt;&lt;/a&gt;. You will need it
later.&lt;/p&gt;

&lt;p&gt;Create a temp working directory and copy your generated server key and
certificate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ mkdir logstash-tmp &amp;amp;&amp;amp; cd logstash-tmp
$ cp xxx/&amp;lt;filename&amp;gt;.crt .
$ cp xxx/&amp;lt;filename&amp;gt;.key .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a &lt;code&gt;logstash.conf&lt;/code&gt; file that corresponds to your key and certificate files.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ echo &amp;lt;&amp;lt;&amp;lt; EOL
input {
  lumberjack {
    port =&amp;gt; 5043
    ssl_certificate =&amp;gt; &amp;quot;/logstash/certs/&amp;lt;filename&amp;gt;.crt&amp;quot;
    ssl_key =&amp;gt; &amp;quot;/logstash/certs/&amp;lt;filename&amp;gt;.key&amp;quot;
    type =&amp;gt; &amp;quot;lumberjack&amp;quot;
  }
}

output {
  elasticsearch {
    hosts =&amp;gt; [&amp;quot;elasticsearch:9200&amp;quot;]
  }
}
EOL &amp;gt;&amp;gt; logstash.conf;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You shold now have three files: &lt;code&gt;&amp;lt;filename&amp;gt;.key &amp;lt;filename&amp;gt;.crt logstash.conf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Create two secrets that will be mounted in the &lt;code&gt;logstash&lt;/code&gt; pod
(see &lt;a href=&#34;https://github.com/larmog/kubernetes-elk-cluster/blob/master/logstash-controller.yaml&#34;&gt;&lt;code&gt;logstash-controller.yaml&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Create logstash-ssl secret
$ conf2kube -n logstash-ssl -f &amp;lt;filename&amp;gt;.crt | kubectl create -f -
$ kubectl patch secret logstash-ssl -p `conf2kube -n logstash-ssl -f &amp;lt;filename&amp;gt;.key`
$
$ # Create logstash.conf secret
$ conf2kube -n logstash.conf -f logstash.conf | kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now have two &lt;code&gt;logstash&lt;/code&gt; secrets:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get secrets
NAME                        TYPE                                  DATA      AGE
default-token-i5v41         kubernetes.io/service-account-token   2         59d
logstash-ssl                Opaque                                2         1d
logstash.conf               Opaque                                1         1d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now clone the kubernetes-elk-cluster repo and create the logstash server and
kibana controller:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/larmog/kubernetes-elk-cluster.git
$ cd kubernetes-elk-cluster
$ kubectl create -f service-account.yaml
$ kubectl create -f logstash-service.yaml
$ kubectl create -f logstash-controller.yaml
$ kubectl create -f kibana-controller.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! You should now have &lt;code&gt;logstash&lt;/code&gt; and &lt;code&gt;kibana&lt;/code&gt; running!&lt;/p&gt;

&lt;p&gt;For you to be able to access &lt;code&gt;kibana&lt;/code&gt;, you need to expose the service outside
your cluster. I assume that you have used the &lt;code&gt;Kubernetes-On-ARM&lt;/code&gt; addon
&lt;code&gt;loadbalancer&lt;/code&gt;. Kibana doesn&amp;rsquo;t work with a sub-context so you need to expose
&lt;code&gt;kibana&lt;/code&gt; as a virtual host.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Run the following command with your Kibana host name.
$ sed -e &#39;s/kibana.kodbasen.org/&amp;lt;hostname&amp;gt;/g&#39; kibana-service.yaml &amp;gt; modified-kibana-service.yaml
$ kubectl create -f modified-kibana-service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If everything has gone according to plan you should see something similar:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl -I http://&amp;lt;hostname&amp;gt;
HTTP/1.1 400 Bad Request
kbn-name: kibana
kbn-version: 4.4.0
content-type: application/json; charset=utf-8
cache-control: no-cache
Date: Sat, 12 Mar 2016 11:16:37 GMT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Never mind the &lt;code&gt;400&lt;/code&gt; answer, you can see that Kibana has replied with
&lt;code&gt;kbn-name: kibana&lt;/code&gt; and &lt;code&gt;kbn-version: 4.4.0&lt;/code&gt;. Now try to open Kibana in your
favorite browser.&lt;/p&gt;

&lt;p&gt;We have covered a lot of ground, and you should give your self a tap on the
shoulder. But were not done yet. We have &lt;em&gt;elasticsearch&lt;/em&gt; running and &lt;em&gt;logstash&lt;/em&gt;
waiting for logs to be stored in elastic. &lt;em&gt;Kibana&lt;/em&gt; is hanging around waiting for
logs to appear in the &lt;code&gt;.kibana&lt;/code&gt; index. But we need to start thinking of how
to forward logs from our nodes to &lt;em&gt;logstash&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&#34;logstash-forwarder&#34;&gt;Logstash Forwarder&lt;/h4&gt;

&lt;p&gt;In this first part we will use &lt;code&gt;logstash-forwarder&lt;/code&gt; to collect logs from our
nodes and forward them to logstash. In the following parts of this series we
will look into other ways. The &lt;code&gt;logstash-forwarder&lt;/code&gt; project has been replaced
by &lt;code&gt;filebeat&lt;/code&gt; but thatÂ´s a topic for the next post.&lt;/p&gt;

&lt;p&gt;We need to run &lt;code&gt;logstash-forwarder&lt;/code&gt; on every node that we wan&amp;rsquo;t to collect logs
from. Kubernetes &lt;code&gt;1.2&lt;/code&gt; will include daemon sets, which is a great feature for
running things on a set of nodes but we are still running &lt;code&gt;1.1.x&lt;/code&gt;. In the
meantime, until &lt;code&gt;1.2&lt;/code&gt; is released, we&amp;rsquo;ll use static pods.&lt;/p&gt;

&lt;p&gt;Repeat the following steps on every node where you wan&amp;rsquo;t to collect logs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ mkdir -p /etc/logstash-forvarder/certs &amp;amp;&amp;amp; mkdir /etc/logstash-forvarder/config
$
$ # Copy the CA certificate that was used to create the logstash cert.
$ cp ca.crt /etc/logstash-forvarder/certs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the &lt;code&gt;logstash-forwarder.conf&lt;/code&gt; in &lt;code&gt;/etc/logstash-forvarder/config&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;network&amp;quot;: {
    &amp;quot;servers&amp;quot;: [
        &amp;quot;logstash.default.svc.cluster.local:5043&amp;quot;
    ],
    &amp;quot;timeout&amp;quot;: 15,
    &amp;quot;ssl ca&amp;quot;: &amp;quot;/logstash-forwarder/certs/ca.crt&amp;quot;
  },
  &amp;quot;files&amp;quot;: [
    {
      &amp;quot;paths&amp;quot;: [
        &amp;quot;/var/log/syslog&amp;quot;
      ],
      &amp;quot;fields&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;syslog&amp;quot;}
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the &lt;code&gt;logstash-forwarder&lt;/code&gt; static pod file &lt;code&gt;logstash-forwarder.json&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
  &amp;quot;kind&amp;quot;: &amp;quot;Pod&amp;quot;,
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;name&amp;quot;:&amp;quot;logstash-forwarder&amp;quot;
  },
  &amp;quot;spec&amp;quot;:{
    &amp;quot;containers&amp;quot;:[
      {
        &amp;quot;name&amp;quot;: &amp;quot;logstash-forwarder&amp;quot;,
        &amp;quot;image&amp;quot;: &amp;quot;larmog/logstash-forwarder:2.2.0&amp;quot;,
        &amp;quot;imagePullPolicy&amp;quot;: &amp;quot;IfNotPresent&amp;quot;,
        &amp;quot;command&amp;quot;: [
        ],
        &amp;quot;securityContext&amp;quot;: {
          &amp;quot;privileged&amp;quot;: true
        },
        &amp;quot;volumeMounts&amp;quot;:[{
          &amp;quot;name&amp;quot;: &amp;quot;certs&amp;quot;,
          &amp;quot;mountPath&amp;quot;: &amp;quot;/logstash-forwarder/certs&amp;quot;
        },
        {
          &amp;quot;name&amp;quot;: &amp;quot;config&amp;quot;,
          &amp;quot;mountPath&amp;quot;: &amp;quot;/logstash-forwarder/config&amp;quot;
        },
        {
          &amp;quot;name&amp;quot;: &amp;quot;syslog&amp;quot;,
          &amp;quot;mountPath&amp;quot;: &amp;quot;/var/log/syslog&amp;quot;
        }]
    }],
    &amp;quot;volumes&amp;quot;: [
      {
        &amp;quot;name&amp;quot;: &amp;quot;certs&amp;quot;,
        &amp;quot;hostPath&amp;quot;: {
          &amp;quot;path&amp;quot;: &amp;quot;/etc/logstash-forwarder/certs&amp;quot;
        }
      },{
        &amp;quot;name&amp;quot;: &amp;quot;config&amp;quot;,
        &amp;quot;hostPath&amp;quot;: {
          &amp;quot;path&amp;quot;: &amp;quot;/etc/logstash-forwarder/config&amp;quot;
        }
      },
      {
        &amp;quot;name&amp;quot;: &amp;quot;syslog&amp;quot;,
        &amp;quot;hostPath&amp;quot;: {
          &amp;quot;path&amp;quot;: &amp;quot;/var/log/syslog&amp;quot;
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Copy the file to the manifest directory. In Kubernetes-On-ARM there located
here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;On the master: &lt;code&gt;/etc/kubernetes/static-pods/master&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;On workers: &lt;code&gt;/etc/kubernetes/static-pods/worker&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;kubelet&lt;/code&gt; will pick up the static pod definition as soon as the file is
copied to the manifets directory. Run &lt;code&gt;docker ps|grep logstash-forwarder:2.2.0&lt;/code&gt;
on the node, and you should see the container running.&lt;/p&gt;

&lt;p&gt;And we are done! The &lt;code&gt;logstash-forwarder&lt;/code&gt; starts processing events and if you
look at the container log, you should see something similar:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker logs -f 8c4a3ba3f3e9
...
2016/03/11 15:13:41.575130 Registrar: processing 1024 events
2016/03/11 15:13:55.862568 Registrar: processing 1024 events
2016/03/11 15:14:01.671923 Registrar: processing 1024 events
2016/03/11 15:14:02.889663 Registrar: processing 256 events
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;to-sum-up&#34;&gt;To sum up&lt;/h4&gt;

&lt;p&gt;We have created a &lt;code&gt;elasticsearch&lt;/code&gt; cluster for storing logs, and &lt;code&gt;logstash&lt;/code&gt;for
processing incoming logs. We&amp;rsquo;ve also setup &lt;code&gt;kibana&lt;/code&gt; for searching and
visualizing. On our nodes we have installed &lt;code&gt;logstash-forwarder&lt;/code&gt;, to send all
&lt;code&gt;/var/log/syslog&lt;/code&gt; events to &lt;code&gt;logstash&lt;/code&gt;, using the &lt;code&gt;lumberjack&lt;/code&gt; protocol.&lt;/p&gt;

&lt;p&gt;In the next part in this series we will investigate how we can forward stdout
and stderr from our containers to &lt;code&gt;logstash&lt;/code&gt;. As always, I hope you picked up
something new and found it worth while reading.&lt;/p&gt;

&lt;h5 id=&#34;docker-images&#34;&gt;Docker images&lt;/h5&gt;

&lt;p&gt;All images are built from Pauls with generated &lt;code&gt;Dockerfile&lt;/code&gt;:s for &lt;code&gt;armhf&lt;/code&gt;.
The &lt;em&gt;Java&lt;/em&gt; base image for &lt;code&gt;elasticsearch&lt;/code&gt; is
&lt;a href=&#34;https://hub.docker.com/r/larmog/armhf-alpine-java/&#34;&gt;larmog/armhf-alpine-java&lt;/a&gt;&lt;/p&gt;

&lt;h5 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pires&#34;&gt;Paulo Pires&lt;/a&gt; repositories for
&lt;code&gt;elasticsearch&lt;/code&gt;, &lt;code&gt;logstash&lt;/code&gt; and &lt;code&gt;kibana&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/leannenorthrop&#34;&gt;Leanne Northrop&lt;/a&gt; repository for Docker
Java image on &lt;code&gt;armhf&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;links&#34;&gt;Links&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pires/kubernetes-elasticsearch-cluster&#34;&gt;https://github.com/pires/kubernetes-elasticsearch-cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pires/kubernetes-elk-cluster&#34;&gt;https://github.com/pires/kubernetes-elk-cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://technologyconversations.com/2015/05/18/centralized-system-and-docker-logging-with-elk-stack/&#34;&gt;http://technologyconversations.com/2015/05/18/centralized-system-and-docker-logging-with-elk-stack/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://thepracticalsysadmin.com/shipping-logs-to-elk/&#34;&gt;https://thepracticalsysadmin.com/shipping-logs-to-elk/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.dasblinkenlichten.com/logging-in-kubernetes-with-fluentd-and-elasticsearch/&#34;&gt;http://www.dasblinkenlichten.com/logging-in-kubernetes-with-fluentd-and-elasticsearch/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://kubernetes.io/v1.1/docs/getting-started-guides/logging-elasticsearch.html&#34;&gt;http://kubernetes.io/v1.1/docs/getting-started-guides/logging-elasticsearch.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://kubernetes.io/v1.1/docs/user-guide/logging.html&#34;&gt;http://kubernetes.io/v1.1/docs/user-guide/logging.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.fluentd.org/guides/recipes/docker-logging&#34;&gt;http://www.fluentd.org/guides/recipes/docker-logging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch/fluentd-es-image&#34;&gt;https://github.com/GoogleCloudPlatform/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch/fluentd-es-image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gliderlabs/logspout&#34;&gt;https://github.com/gliderlabs/logspout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/looplab/logspout-logstash&#34;&gt;https://github.com/looplab/logspout-logstash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/gliderlabs/logspout/~/dockerfile/&#34;&gt;https://hub.docker.com/r/gliderlabs/logspout/~/dockerfile/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://groups.google.com/forum/#!topic/google-containers/ZJzUIn_r3w4&#34;&gt;https://groups.google.com/forum/#!topic/google-containers/ZJzUIn_r3w4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;old &lt;a href=&#34;https://github.com/stuart-warren/logstash-input-journald&#34;&gt;https://github.com/stuart-warren/logstash-input-journald&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/logstash-plugins/logstash-input-journald&#34;&gt;https://github.com/logstash-plugins/logstash-input-journald&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/elastic/logstash/issues/1729&#34;&gt;https://github.com/elastic/logstash/issues/1729&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vaijab/logstash-filter-kubernetes&#34;&gt;https://github.com/vaijab/logstash-filter-kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.fluentd.org&#34;&gt;http://www.fluentd.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>GlusterFS On Kubernetes ARM</title>
      <link>http://larmog.github.io/2016/02/22/glusterfs-on-kubernetes-arm/</link>
      <pubDate>Mon, 22 Feb 2016 09:35:30 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/22/glusterfs-on-kubernetes-arm/</guid>
      <description>&lt;p&gt;If you followed my earlier posts, you know that Iâm running a Kubernetes cluster
on Raspberry Pi, using &lt;strong&gt;HypriotOS&lt;/strong&gt; and Lucas KÃ¤ldstrÃ¶ms &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes-On-ARM&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;If not you can find my earlier posts here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;Kubernetes-On-ARM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/&#34;&gt;Gogs and Drone On Kubernetes-ARM - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/&#34;&gt;Gogs and Drone On Kubernetes-ARM - Part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-3/&#34;&gt;Gogs and Drone On Kubernetes-ARM - Part 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;ve used &lt;strong&gt;NFS&lt;/strong&gt; volumes for storage and DiskStation NAS as NFS server.
The solution have had some drawbacks and it&amp;rsquo;s been hard to synchronize users and
groups, and I&amp;rsquo;ve used &lt;code&gt;all_squash&lt;/code&gt; to a specific &lt;code&gt;uid&lt;/code&gt;. That worked for &lt;strong&gt;Gogs&lt;/strong&gt;
and &lt;strong&gt;Drone&lt;/strong&gt; but not for &lt;strong&gt;MySQL&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Another alternative is to use &lt;strong&gt;GlusterFS&lt;/strong&gt; volumes. GlusterFS is a clustered
file-system that is capable of scaling to several peta-bytes.
GlusterFS aggregates storage bricks, that can be made of commodity hardware.
When I started this project, I had two
&lt;code&gt;Raspberry Pi 1 B+&lt;/code&gt; in my desk drawer. I&amp;rsquo;ve used them as nodes in the cluster,
but the small amount of &lt;code&gt;RAM&lt;/code&gt; has shown them less useful compared to the newer
&lt;code&gt;Raspberry Pi 2 B&lt;/code&gt; based nodes. But then it hit me that maybe I could use the
old Pi 1 boards as bricks in GlusterFS. One of the purposes with this project is
to see how much of a Kubernetes cluster you can get for a reasonable amount. So
I&amp;rsquo;d like to keep a tight budget. Normally when setting up a brick you want to
use RAID, because you don&amp;rsquo;t want to handle disk failures on a brick level, but
for this project it&amp;rsquo;s a reasonable solution.&lt;/p&gt;

&lt;p&gt;After a trip to my local Electronic Retailers I came home with this
(not the cluster):

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;GlusterFS ingredients&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/IMG_1953.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;* 2 WD external usb harddrives&lt;/li&gt;
&lt;li&gt;* 2 Raspberry Pi 2 B boards (to replace the Pi 1:s)&lt;/li&gt;
&lt;li&gt;* 2 SD cards&lt;/li&gt;
&lt;li&gt;* One more Multi-Pi Stackable Raspberry Pi Case&lt;/li&gt;
&lt;li&gt;* Cables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My first attempt was to connect the external drives to the usb port. That didn&amp;rsquo;t
work of course, external disks needs more power than what the Pi usb port can
deliver. There are two alternatives for solving this. You could use an external
usb-hub with it&amp;rsquo;s own power supply. But that means two usb-hubs and two power
adapters. The other alternative is to use a usb Y-cable and connect the extra
connector to the usb supercharger (that powers all nodes). After one more trip
to the store to get a Y-cable and then connect it to the usb supercharger, the
problem was solved.&lt;/p&gt;

&lt;p&gt;The image shows the cluster after assembling.

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Cluster ready for GlusterFS&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/IMG_1961.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s room for two more Pi:s in the switch and usb supercharger.&lt;/p&gt;

&lt;p&gt;Next step was to partition and format the disks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Prepare your disk and create a partition first
$ # Install xfs tools
$ apt-get install -y xfsprogs
$ # Format the disk
$ mkfs.xfs -f -L brick1 -i size=512 /dev/sda1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and mount and install &lt;code&gt;glusterfs&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Create and mount the brick
$ mkdir -p /data/brick1
$ echo &#39;/dev/sda1 /data/brick1 xfs defaults 1 2&#39; &amp;gt;&amp;gt; /etc/fstab
$ mount -a &amp;amp;&amp;amp; mount
# Install GlusterFS server
$ apt-get install -y glusterfs-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The steps above was made on both servers.&lt;/p&gt;

&lt;p&gt;Next we needed to connect our storage servers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ #From server1
$ gluster peer probe server2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ #From server2
$ gluster peer probe server1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Time to create the first volume:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ mkdir /data/brick1/vol0
$ gluster volume create vol0 replica 2 store1.local:/data/brick1/vol0 store2.local:/data/brick2/vol0
$ gluster volume start vol0
$ gluster volume info

Volume Name: vol0
Type: Replicate
Volume ID: 6ffe7723-e0ac-44c7-a004-fa951467884d
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: server1:/data/brick1/vol0
Brick2: server2:/data/brick2/vol0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next you need to install &lt;code&gt;glusterfs-client&lt;/code&gt; on all your Kubernetes nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ apt-get install -y glusterfs-client
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done.&lt;/p&gt;

&lt;p&gt;The only thing left is to set up our &lt;code&gt;glusterfs-cluster endpoints&lt;/code&gt; as described
in the &lt;a href=&#34;http://kubernetes.io/v1.1/examples/glusterfs/README.html&#34;&gt;Kubernetes documentation for GlusterFS Volumes&lt;/a&gt; and configure
our &lt;a href=&#34;http://kubernetes.io/v1.1/docs/user-guide/persistent-volumes.html&#34;&gt;Persistent Volumes and Claims&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Migrating the volumes from NFS to GlusterFS was very easy. The only problem
I&amp;rsquo;ve had was a silly mistake. Coming from NFS I first tried to share a volume
between applications, using different paths: &lt;code&gt;/vol 0/mysql&lt;/code&gt;, &lt;code&gt;/vol/drone&lt;/code&gt; that
didn&amp;rsquo;t work. You need to create different volumes for each application. Except
from the small misconception, there&amp;rsquo;s been no problems at all.&lt;/p&gt;

&lt;p&gt;Now finally I have a Kubernetes cluster running on bare metal that can be used
for trying out techniques, for building microservices.&lt;/p&gt;

&lt;p&gt;By the way, I took the opportunity to overclock my Raspberry Pi:s.
Thanks to &lt;a href=&#34;http://haydenjames.io/raspberry-pi-2-overclock/&#34;&gt;Hayden James&lt;/a&gt;, all
cluster nodes now runs at 1000 Mhz, and it seems stable.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes On ARM</title>
      <link>http://larmog.github.io/2016/02/06/kubernetes-on-arm/</link>
      <pubDate>Sat, 06 Feb 2016 10:33:08 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/06/kubernetes-on-arm/</guid>
      <description>&lt;p&gt;I really like Kubernetes for orchestrating Docker containers. If you&amp;rsquo;re don&amp;rsquo;t
familiar with Kubernetes I can highly recommend to take a look at
&lt;a href=&#34;http://kubernetes.io&#34;&gt;kubernetes.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can easily run you&amp;rsquo;re own Kubernetes cluster on your local machine using
Vagrant or run Kubernetes in the cloud using AWS, Azure or Google Compute.
But I like a more hands on solution and I always wanted my own &amp;ldquo;data center&amp;rdquo;.
On the other hand I don&amp;rsquo;t want to spend a fortune building a DC just for fun.&lt;/p&gt;

&lt;p&gt;A great alternative is to use ARM SoC boards like Raspberry PI. Thank&amp;rsquo;s to
&lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas KÃ¤ldstrÃ¶m&lt;/a&gt; and
&lt;a href=&#34;http://blog.hypriot.com/&#34;&gt;Hypriot&lt;/a&gt; this is a rather straight forward process.&lt;/p&gt;

&lt;p&gt;I used the &lt;code&gt;hypriotos&lt;/code&gt; image and Lucas &lt;code&gt;deb&lt;/code&gt;-package to install &lt;code&gt;kube-config&lt;/code&gt;.&lt;/p&gt;


&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Six Raspberry Pi:s in a cluster&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/IMG_1936.png&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;




&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Nodes in my cluster&lt;/span&gt;
          &lt;p&gt;
          Here you can see the nodes in the cluster. I&amp;#39;m using the service-loadbalancer addon (a topic for another post).
          
              
          
        &lt;/p&gt; 
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/k8s.png&#34; alt=&#34;Here you can see the nodes in the cluster. I&amp;#39;m using the service-loadbalancer addon (a topic for another post).&#34; /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;


</description>
    </item>
    
  </channel>
</rss>