<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Development on kodbasen</title>
    <link>http://larmog.github.io/categories/development/</link>
    <description>Recent content in Development on kodbasen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright © 2016 by larmog</copyright>
    <lastBuildDate>Thu, 14 Jul 2016 19:16:43 +0200</lastBuildDate>
    <atom:link href="http://larmog.github.io/categories/development/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Building your Kubernetes cluster in minutes</title>
      <link>http://larmog.github.io/2016/07/14/building-your-kubernetes-cluster-in-minutes/</link>
      <pubDate>Thu, 14 Jul 2016 19:16:43 +0200</pubDate>
      
      <guid>http://larmog.github.io/2016/07/14/building-your-kubernetes-cluster-in-minutes/</guid>
      <description>&lt;p&gt;With the release of Kubernetes &lt;code&gt;v1.3.0&lt;/code&gt; there is now a cross platform way of
quickly setting up a Kubernetes cluster using Docker.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kube-deploy/tree/master/docker-multinode&#34;&gt;kube-deploy/docker-multinode&lt;/a&gt;
uses two instances of Docker daemon.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve created a hybrid solution &lt;a href=&#34;https://github.com/kodbasen/sloop&#34;&gt;Sloop&lt;/a&gt; that uses kube-deploy/docker-multinode but runs
the kubelet &lt;em&gt;native&lt;/em&gt;. The reason for running your &lt;code&gt;kubelet&lt;/code&gt;:s &lt;em&gt;native&lt;/em&gt; is the full support of &lt;a href=&#34;http://kubernetes.io/docs/user-guide/persistent-volumes/&#34;&gt;Persistent Volumes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Sloop uses the best of two worlds. Docker for bootstrapping your cluster and &lt;em&gt;native&lt;/em&gt; kubelet for full Kubernetes functionality.&lt;/p&gt;

&lt;p&gt;Give it a try and see what you think!
&lt;a href=&#34;https://github.com/kodbasen/sloop&#34;&gt;https://github.com/kodbasen/sloop&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ingress Resources, the final piece of the puzzle?</title>
      <link>http://larmog.github.io/2016/05/19/ingress-resources-the-final-piece-of-the-puzzle/</link>
      <pubDate>Thu, 19 May 2016 13:29:40 +0200</pubDate>
      
      <guid>http://larmog.github.io/2016/05/19/ingress-resources-the-final-piece-of-the-puzzle/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve been using service-loadbalancer in my cluster and annotated my services to
make them available outside the cluster network. With the release of Kubernetes
1.2 there is another alternative, the Ingress Resources.&lt;/p&gt;

&lt;p&gt;You can read more about the Ingress resource in the &lt;a href=&#34;http://kubernetes.io/docs/user-guide/ingress/&#34;&gt;Kubernetes Reference Documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve always felt that there&amp;rsquo;s been something missing in Kubernetes when it comes
to exposing services. There has been multiple ways of doing it, and it&amp;rsquo;s
been up to you or your Kubernetes provider to decide how. You can use the
&lt;code&gt;service-loadbalancer&lt;/code&gt; method but it&amp;rsquo;s a bit messy. You need to keep
track of your annotations, and I personally think that they shouldn&amp;rsquo;t be there
in the first place. With the Ingress Resource you get a clear separation between
your service, and how you expose the service outside your cluster.&lt;/p&gt;

&lt;h4 id=&#34;ingress-controller:ebff967dba27d185454e1bb8a30150d3&#34;&gt;Ingress controller&lt;/h4&gt;

&lt;p&gt;Creating an Ingress without an Ingress controller will have no effect. So we
first need to install a controller. You can read more about Ingress controllers
&lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/ingress/controllers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is a &lt;code&gt;nginx-ingress-controller&lt;/code&gt; in the
&lt;a href=&#34;https://github.com/kubernetes/contrib&#34;&gt;contrib&lt;/a&gt; repository and an image
&lt;code&gt;gcr.io/google_containers/nginx-ingress-controller&lt;/code&gt;. If you read any of my
earlier &lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;posts&lt;/a&gt;, then you know
that I&amp;rsquo;m running &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;&lt;code&gt;Kubernetes-On-ARM&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is no official &lt;code&gt;nginx-ingress-controller-arm&lt;/code&gt; image yet, but if you want
you can use mine. I&amp;rsquo;ve built the same image chain as the
&lt;code&gt;nginx-ingress-controller&lt;/code&gt;, but for &lt;code&gt;armhf&lt;/code&gt;. Here&amp;rsquo;s the image chain:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/kodbasen/ubuntu-slim-armhf/&#34;&gt;https://hub.docker.com/r/kodbasen/ubuntu-slim-armhf/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/kodbasen/nginx-slim-armhf/&#34;&gt;https://hub.docker.com/r/kodbasen/nginx-slim-armhf/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/kodbasen/nginx-ingress-controller-armhf/&#34;&gt;https://hub.docker.com/r/kodbasen/nginx-ingress-controller-armhf/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;ve also built the &lt;code&gt;defaultbackend-armhf&lt;/code&gt; image:
&lt;a href=&#34;https://hub.docker.com/r/kodbasen/defaultbackend-armhf/&#34;&gt;https://hub.docker.com/r/kodbasen/defaultbackend-armhf/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All you need to do is to change the images in the &lt;code&gt;rc.yaml&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget -q https://raw.githubusercontent.com/kubernetes/contrib/master/ingress/controllers/nginx/rc.yaml \
&amp;amp;&amp;amp; sed -i -e &amp;quot;s;gcr\.io/google_containers/nginx-ingress-controller;kodbasen/nginx-ingress-controller-armhf;&amp;quot; &amp;quot;rc.yaml&amp;quot; \
&amp;amp;&amp;amp; sed -i -e &amp;quot;s;gcr\.io/google_containers/defaultbackend;kodbasen/defaultbackend-armhf;&amp;quot; &amp;quot;rc.yaml&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now your ready to deploy your &lt;code&gt;nginx-ingress-controller-armhf&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl create -f rc.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it, you can now start experimenting with &lt;strong&gt;Ingress Resources&lt;/strong&gt; on
&lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;&lt;code&gt;Kubernetes-On-ARM&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Ingress Resources is maybe the final piece of the puzzle, to make Kubernetes
totally awesome. What do you think?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Storage for your cluster</title>
      <link>http://larmog.github.io/2016/05/12/storage-for-your-cluster/</link>
      <pubDate>Thu, 12 May 2016 06:48:19 +0200</pubDate>
      
      <guid>http://larmog.github.io/2016/05/12/storage-for-your-cluster/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve been working on getting Elasticsearch working on Kubernetes-On-ARM. The
biggest problem has been storage. I&amp;rsquo;m using Elasticserach for storing logs and
the cluster generates 1.4 million entries per day (i know, need to do
something about it).

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Hits during 24 hours&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/es-hits.png&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;


If you want to deploy your applications to Kubernetes, sooner or later you need
to think about how to solve storage and persistence.&lt;/p&gt;

&lt;p&gt;Kubernetes is a distributed cluster where nodes and pods comes and goes. We
don&amp;rsquo;t want to solve our persistence problem for a single node, we want to solve
it for the whole cluster. That&amp;rsquo;s where Kubernetes Volumes comes in. A quick look
at the different &lt;a href=&#34;http://kubernetes.io/docs/user-guide/volumes/&#34;&gt;Volume Plugins&lt;/a&gt;
that are available gives us the following alternatives:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nfs&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iscsi&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;glusterfs&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wrote an earlier post about
&lt;a href=&#34;http://larmog.github.io/2016/02/22/glusterfs-on-kubernetes-arm/&#34;&gt;GlusterFS On Kubernetes-ARM&lt;/a&gt;.
I still use &lt;code&gt;glustefs&lt;/code&gt; and it&amp;rsquo;s running on my &lt;code&gt;rpi-1&lt;/code&gt;
boards, but the I/O performance isn&amp;rsquo;t enough for handling the logs from
Fluentd that are stored in Elasticsearch. We need a bigger machinery.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been looking for a board that have Gigabit Ethernet and SATA or
USB 3, that can be used for handling persistence. But buying a board and some
disks and configuring services, that sounds a lot like building your own NAS,
and there are plenty of cheap NAS products out there that supports at least two
of the alternatives on our list, &lt;code&gt;nfs&lt;/code&gt; and &lt;code&gt;iscsi&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I already own a home &lt;code&gt;NAS&lt;/code&gt; and it supports both &lt;code&gt;nfs&lt;/code&gt; and &lt;code&gt;iscsi&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I created two different &lt;code&gt;PersistentVolume&lt;/code&gt;:s, one for each Volume Plugin, and
mounted them in two different Elasticsearch data node pods, and suddenly, all the
problems with Elasticsearch and Kibana is gone.&lt;/p&gt;

&lt;h5 id=&#34;using-iscsi:28bd4339389ee1959293ee1ce0b1ef71&#34;&gt;Using iSCSI&lt;/h5&gt;

&lt;p&gt;Installing &lt;code&gt;iSCSI&lt;/code&gt; Initiator in Arch Linux is rather straight forward:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ pacman -S --noconfirm open-iscsi
$ systemctl enable open-iscsi.service
$ systemctl restart open-iscsi.service
$ iscsiadm -m discovery -t sendtargets -p &amp;lt;ip of your iscsi target&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You need to repeat the steps above for each node in your cluster and that&amp;rsquo;s it,
if your not going to use &lt;code&gt;CHAP&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now we can create a &lt;code&gt;PersistentVolume&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: lun0
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  iscsi:
    targetPortal: xxx.xxx.xxx.xxx:yyyy
    iqn: &amp;lt;iqn-target&amp;gt;
    lun: 0
    fsType: ext4
    readOnly: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I leave it up to you to create a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; and mount it in to your
pod.&lt;/p&gt;

&lt;h4 id=&#34;to-sum-up:28bd4339389ee1959293ee1ce0b1ef71&#34;&gt;To sum up&lt;/h4&gt;

&lt;p&gt;If you want to use your Kubernetes-On-ARM cluster for more advanced applications
then you need to think about how to solve persistence.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve found a solution that suits my needs using &lt;code&gt;iscsi&lt;/code&gt; and &lt;code&gt;nfs&lt;/code&gt;, the only
problem is that my NAS is almost full, and not so portable.&lt;/p&gt;

&lt;p&gt;Maybe it&amp;rsquo;s time to build
&lt;code&gt;Kodbasen cluster version 2&lt;/code&gt;?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>E(F)K cluster on Kubernetes-On-ARM - Part 2</title>
      <link>http://larmog.github.io/2016/05/02/efk-cluster-on-kubernetes-on-arm---part-2/</link>
      <pubDate>Mon, 02 May 2016 08:55:37 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/05/02/efk-cluster-on-kubernetes-on-arm---part-2/</guid>
      <description>

&lt;p&gt;This is the second part in a series about handling logs on Kubernetes-On-ARM.
In the first part we installed ELK and started sending syslog events from our
nodes using &lt;code&gt;logstash-forwarder&lt;/code&gt;. In this part we will start collecting logs
from our &lt;code&gt;pods&lt;/code&gt; and Kubernetes components. If you wan&amp;rsquo;t to cache up here&amp;rsquo;s a
list of previous posts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;Kubernetes-On-ARM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/03/13/elk-cluster-on-kubernetes-on-arm---part-1/&#34;&gt;ELK cluster on Kubernetes-On-ARM - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The plan was that this part would be about how
to start collecting logs from Kubernetes. But I wasn&amp;rsquo;t satisfied with how
&lt;code&gt;logstash-forwarder&lt;/code&gt; worked. The thing is that, once the &lt;code&gt;logstash-forwarder&lt;/code&gt;
daemon is started, the node can&amp;rsquo;t run much else.&lt;/p&gt;

&lt;p&gt;Another problem with the first attempt was that we were collecting &lt;code&gt;syslog&lt;/code&gt;
messages instead of collecting logs directly from &lt;code&gt;journald&lt;/code&gt;. There is no
&lt;code&gt;syslog&lt;/code&gt; on &lt;code&gt;Arch Linux&lt;/code&gt; by default. You need to install &lt;code&gt;syslog-ng&lt;/code&gt; in order to
make it available. That doesn&amp;rsquo;t make sense when running on a &lt;code&gt;Raspberry-Pi&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A better solution is to collect the logs directly from the source, and send them
to &lt;code&gt;elasticsearch&lt;/code&gt;. In this second part we will replace &lt;code&gt;logstash-forwarder&lt;/code&gt; and
&lt;code&gt;logstash&lt;/code&gt; with &lt;code&gt;fluentd&lt;/code&gt; for collecting logs. We will add two &lt;code&gt;systemd&lt;/code&gt;
services on each node that will keep track of the position in the &lt;code&gt;journald&lt;/code&gt; and
send logs as &lt;code&gt;JSON&lt;/code&gt; to &lt;code&gt;fluentd&lt;/code&gt; using &lt;code&gt;ncat&lt;/code&gt;. Fluentd will receive the logs
from our service and also collect logs from &lt;code&gt;docker&lt;/code&gt; containers on each host
and add metadata from &lt;code&gt;Kubernetes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s get started :)&lt;/p&gt;

&lt;p&gt;If you haven&amp;rsquo;t upgraded to &lt;code&gt;Kubernetes-On-ARM v0.7.0&lt;/code&gt; and &lt;code&gt;Kubernetes 1.2&lt;/code&gt;, do
that first.&lt;/p&gt;

&lt;h4 id=&#34;upgrading-to-kibana-4-5-0-and-elasticsearch-2-3-2:067b7f7d4fc16bce7784c9369d044678&#34;&gt;Upgrading to Kibana 4.5.0 and Elasticsearch 2.3.2&lt;/h4&gt;

&lt;p&gt;I have created images for &lt;code&gt;elasticsearch&lt;/code&gt; and &lt;code&gt;kibana&lt;/code&gt; that can be found here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/kodbasen/elasticsearch-kubernetes-armhf/&#34;&gt;https://hub.docker.com/r/kodbasen/elasticsearch-kubernetes-armhf/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/kodbasen/kibana-armhf/&#34;&gt;https://hub.docker.com/r/kodbasen/kibana-armhf/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to upgrade &lt;code&gt;elasticsearch&lt;/code&gt; to version &lt;code&gt;2.3.2&lt;/code&gt; and &lt;code&gt;kibana&lt;/code&gt; version
&lt;code&gt;4.5.0&lt;/code&gt; run the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Warning this will remove your elasticsearch from Part 1
$ kubectl delete -f https://raw.githubusercontent.com/kodbasen/elasticsearch-kubernetes-armhf/master/elasticsearch.yaml
$ kubectl create -f https://raw.githubusercontent.com/kodbasen/elasticsearch-kubernetes-armhf/master/elasticsearch.yaml
$ # Warning this will delete your kibana from Part 1
$ kubectl delete -f https://raw.githubusercontent.com/kodbasen/kibana-armhf/master/kibana.yaml
$ kubectl create -f https://raw.githubusercontent.com/kodbasen/kibana-armhf/master/kibana.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;collecting-journald-logs:067b7f7d4fc16bce7784c9369d044678&#34;&gt;Collecting journald logs&lt;/h4&gt;

&lt;p&gt;I found this solution from &lt;a href=&#34;https://github.com/ianblenke/docker-fluentd/tree/master/systemd&#34;&gt;ianblenke&lt;/a&gt;
and I have created a repo here: &lt;a href=&#34;https://github.com/kodbasen/fluentd-kubernetes&#34;&gt;fluentd-kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how to install the services on each node in your cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Install ncat first (apt-get install nmap or pacman -S nmap)
$ # Clone the repo
$ git clone https://github.com/kodbasen/fluentd-kubernetes &amp;amp;&amp;amp; cd fluentd-kubernetes
$ sudo cp systemd/*.service /usr/lib/systemd/system
$ sudo systemctl enable journald-fluentd-pos.service
$ sudo systemctl restart journald-fluentd-pos.service
$ sudo systemctl enable journald-fluentd.service
$ sudo systemctl restart journald-fluentd.service
$ # You can verify that the streaming works by using the following cmd:
$ ncat -l -k 5170
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;journald-fluentd-pos.service&lt;/code&gt; will keep track of the &lt;code&gt;journald&lt;/code&gt; position
and &lt;code&gt;journald-fluentd.service&lt;/code&gt; will follow &lt;code&gt;journald&lt;/code&gt; in &lt;code&gt;JSON&lt;/code&gt; format and send
the logs to &lt;code&gt;fluentd&lt;/code&gt; on port &lt;code&gt;5170&lt;/code&gt; using &lt;code&gt;ncat&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The next step is to start our &lt;code&gt;daemonSet&lt;/code&gt;, that will collect all logs on the node
and send them to &lt;code&gt;elasticsearch&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/kodbasen/fluentd-kubernetes &amp;amp;&amp;amp; cd fluentd-kubernetes
$ kubectl create -f fluentd-ds.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now see how your &lt;code&gt;fluentd&lt;/code&gt; daemons are starting on all your nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods
NAME                      READY     STATUS    RESTARTS   AGE
es-client-nrlbv           1/1       Running   0          2h
es-client-tm618           1/1       Running   0          2h
es-data-bvsqx             1/1       Running   0          2h
es-master-skdmf           1/1       Running   0          2h
fluentd-319fq             1/1       Running   0          16h
fluentd-41sml             1/1       Running   0          16h
fluentd-5kzt6             1/1       Running   0          16h
fluentd-86scv             1/1       Running   0          16h
fluentd-gqxr8             1/1       Running   0          16h
fluentd-rkod8             1/1       Running   0          16h
fluentd-u0vhl             1/1       Running   0          16h
fluentd-uqx3g             1/1       Running   0          16h
fluentd-zs06d             1/1       Running   0          16h
kibana-6rqgk              1/1       Running   3          2d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our &lt;code&gt;fluentd&lt;/code&gt; daemon accepts incoming logs on &lt;code&gt;tcp&lt;/code&gt; port &lt;code&gt;5170&lt;/code&gt; and tags them
with &lt;code&gt;systemd&lt;/code&gt; it will also tail all logs found in &lt;code&gt;/var/log/containers&lt;/code&gt; and
tag them with &lt;code&gt;kubelet.*&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Open Kibana in your browser and you should see that your logs are beeing stored
in &lt;code&gt;elasticsearch&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve created two saved searches, one foreach tag:&lt;/p&gt;

&lt;p&gt;
&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Systemd tag search in Kibana&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/systemd-tag.png&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;



&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Kubelet tag search in Kibana&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/kubelet-tag.png&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;h4 id=&#34;to-sum-up:067b7f7d4fc16bce7784c9369d044678&#34;&gt;To sum up&lt;/h4&gt;

&lt;p&gt;Now we can collect logs from both &lt;code&gt;PODS&lt;/code&gt; and &lt;code&gt;nodes&lt;/code&gt;. But you will soon discover
that our nodes generates a huge amount of logs. Basically were running the same
software as on the &lt;code&gt;x86_64&lt;/code&gt; platform but with much less resources
(memory, cpu and disk). The old saying &lt;strong&gt;Silence is golden&lt;/strong&gt; is very true
when it comes to logging. Our applications and components should only report
when something is wrong, and even then, the log should be a single line carrying
just enough information to identify the problem. If you take a look at the
stacktraces from &lt;code&gt;elasticsearch&lt;/code&gt; and &lt;code&gt;Java&lt;/code&gt; that is written to &lt;code&gt;stderr&lt;/code&gt;, each
line generates a log message. A single error, written to &lt;code&gt;stderr&lt;/code&gt;, with a
stacktrace can result in a huge amount of log messages. I leave it up to you to
filter out unnecessary logs and aggregate and combine multiple rows in to one
log message. All to minimize the noise.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ELK cluster on Kubernetes-On-ARM - Part 1</title>
      <link>http://larmog.github.io/2016/03/13/elk-cluster-on-kubernetes-on-arm---part-1/</link>
      <pubDate>Sun, 13 Mar 2016 22:44:21 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/03/13/elk-cluster-on-kubernetes-on-arm---part-1/</guid>
      <description>

&lt;p&gt;One of the most important parts of running a cluster is to gain knowledge of
whats going on. Using tools like &lt;code&gt;kubectl logs&lt;/code&gt; or &lt;code&gt;docker logs&lt;/code&gt; is fine if you
run one or two nodes, but it soon gets impossible to get an overview of whats
going on, and you need to be able to view, query and monitor your logs from one
central place.&lt;/p&gt;

&lt;p&gt;This blog post is about setting up
&lt;em&gt;Elasticsearch + Logstash + Kibana ELK on Kubernetes-On-ARM&lt;/em&gt;. You could question
the use of running &lt;em&gt;ELK&lt;/em&gt; on an &lt;em&gt;ARM&lt;/em&gt; cluster of &lt;em&gt;Raspberry Pi&lt;/em&gt;:s, and you are
right, it is a bit to heavy. But I use the cluster as a way of learning, and the
steps are the same as if you are running on &lt;code&gt;x86_64&lt;/code&gt; or in the cloud. There’s also
a bunch of new &lt;code&gt;ARM64&lt;/code&gt; single-board computers on the way, &lt;em&gt;Raspberry Pi 3&lt;/em&gt;
being the first.&lt;/p&gt;

&lt;p&gt;You can use &lt;a href=&#34;http://www.fluentd.org/&#34;&gt;&lt;code&gt;fluentd&lt;/code&gt;&lt;/a&gt; instead of &lt;code&gt;logstash&lt;/code&gt; but we
stick to &lt;code&gt;logstash&lt;/code&gt; for now.&lt;/p&gt;

&lt;p&gt;Setting up &lt;em&gt;ELK&lt;/em&gt; on Kubernetes is nothing new, I&amp;rsquo;m using Paulo Pires:s
&lt;a href=&#34;https://github.com/larmog/kubernetes-elasticsearch-cluster&#34;&gt;kubernetes-elasticsearch-cluster&lt;/a&gt;
and &lt;a href=&#34;https://github.com/larmog/kubernetes-elk-cluster&#34;&gt;kubernetes-elk-cluster&lt;/a&gt;,
modified for ARM.&lt;/p&gt;

&lt;p&gt;The thing is, how do we collect our container logs and send them to logstash?
The first alternative is to use
&lt;a href=&#34;https://github.com/elastic/logstash-forwarder&#34;&gt;logstash-forwarder&lt;/a&gt; but the
project has been replaced by
&lt;a href=&#34;https://github.com/elastic/beats/tree/master/filebeat&#34;&gt;filebeat&lt;/a&gt;, so that
becomes our second alternative. Then I found &lt;em&gt;gliderlabs&lt;/em&gt;
&lt;a href=&#34;https://github.com/gliderlabs/logspout&#34;&gt;logspout&lt;/a&gt; project and &lt;em&gt;looplab&lt;/em&gt;:s
&lt;a href=&#34;https://github.com/looplab/logspout-logstash&#34;&gt;logspout-logstash&lt;/a&gt; module, which
is a third alternative.&lt;/p&gt;

&lt;p&gt;If you want to use &lt;code&gt;kubectl logs&lt;/code&gt; or &lt;code&gt;docker logs&lt;/code&gt; you
need to stick to the &lt;code&gt;json-file&lt;/code&gt; or &lt;code&gt;journald&lt;/code&gt; logging driver in Docker. The
&lt;code&gt;json-file&lt;/code&gt; driver is rather resource consuming and not a great alternative for
production. There&amp;rsquo;s a logstash plugin for
&lt;a href=&#34;https://github.com/logstash-plugins/logstash-input-journald&#34;&gt;&lt;code&gt;journald&lt;/code&gt;&lt;/a&gt; that
looks promising and I hope to explore the different alternatives in the up
coming posts. Using &lt;code&gt;journald&lt;/code&gt; for Docker means that we can use the same
mechanism for sending logs for hosts and containers.&lt;/p&gt;

&lt;p&gt;To summarize what we want to achieve:
&lt;div class=&#34;card-panel teal&#34;&gt;&lt;span class=&#34;white-text&#34;&gt;
&lt;em&gt;We want to collect the logs and collect
them as quick as possible, but still be able to use the tools we&amp;rsquo;re used to like
&lt;code&gt;docker logs&lt;/code&gt; and &lt;code&gt;kubectl logs&lt;/code&gt;.&lt;/em&gt;
&lt;/span&gt;&lt;/div&gt;
We&amp;rsquo;ll see how much of this we can achieve.&lt;/p&gt;

&lt;p&gt;But we need to start somewhere, so let&amp;rsquo;s get started with &lt;code&gt;logstash-forwarder&lt;/code&gt;
and collect &lt;code&gt;syslog&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;elasticsearch:5ef3c2c0e23696371af99bf964485763&#34;&gt;Elasticsearch&lt;/h4&gt;

&lt;p&gt;First we need to get &lt;code&gt;elasticsearch&lt;/code&gt; up and running. I have prepared all
necessary docker images so all you need to do is to deploy them to your cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Clone the git repo
$ git clone https://github.com/larmog/kubernetes-elasticsearch-cluster.git
$ cd kubernetes-elasticsearch-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Follow Paulo&amp;rsquo;s instructions to &lt;a href=&#34;https://github.com/larmog/kubernetes-elasticsearch-cluster#deploy&#34;&gt;deploy&lt;/a&gt;
the cluster. I recommend that you wait until each component is provisioned
before you deploy the next.&lt;/p&gt;

&lt;p&gt;If all went well, you should now have &lt;code&gt;elasticsearch&lt;/code&gt; running. You can check the
status using this command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run --rm --dns=10.0.0.10 hypriot/armhf-busybox wget -q -O -  http://elasticsearch:9200/_cluster/health?pretty
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;logstash-kibana:5ef3c2c0e23696371af99bf964485763&#34;&gt;Logstash + Kibana&lt;/h4&gt;

&lt;p&gt;Paulo’s solution uses the Lumberjack secure protocol, and you need generate your
own certificate and key for &lt;code&gt;logstash.default.svc.cluster.local&lt;/code&gt;. I am using
&lt;code&gt;easyrsa3&lt;/code&gt;. Make sure that you have valid &lt;code&gt;.key&lt;/code&gt; and &lt;code&gt;.crt&lt;/code&gt; files. You can of
course change your protocol to something other than &lt;code&gt;lumberjack&lt;/code&gt; and make the
corresponding change in &lt;code&gt;logstash-forwarder&lt;/code&gt;. I will show you how to setup
&lt;code&gt;lumberjack&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Install Kelsey Hightower:s tool
&lt;a href=&#34;https://github.com/kelseyhightower/conf2kube&#34;&gt;&lt;code&gt;conf2kube&lt;/code&gt;&lt;/a&gt;. You will need it
later.&lt;/p&gt;

&lt;p&gt;Create a temp working directory and copy your generated server key and
certificate:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ mkdir logstash-tmp &amp;amp;&amp;amp; cd logstash-tmp
$ cp xxx/&amp;lt;filename&amp;gt;.crt .
$ cp xxx/&amp;lt;filename&amp;gt;.key .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a &lt;code&gt;logstash.conf&lt;/code&gt; file that corresponds to your key and certificate files.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ echo &amp;lt;&amp;lt;&amp;lt; EOL
input {
  lumberjack {
    port =&amp;gt; 5043
    ssl_certificate =&amp;gt; &amp;quot;/logstash/certs/&amp;lt;filename&amp;gt;.crt&amp;quot;
    ssl_key =&amp;gt; &amp;quot;/logstash/certs/&amp;lt;filename&amp;gt;.key&amp;quot;
    type =&amp;gt; &amp;quot;lumberjack&amp;quot;
  }
}

output {
  elasticsearch {
    hosts =&amp;gt; [&amp;quot;elasticsearch:9200&amp;quot;]
  }
}
EOL &amp;gt;&amp;gt; logstash.conf;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You shold now have three files: &lt;code&gt;&amp;lt;filename&amp;gt;.key &amp;lt;filename&amp;gt;.crt logstash.conf&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Create two secrets that will be mounted in the &lt;code&gt;logstash&lt;/code&gt; pod
(see &lt;a href=&#34;https://github.com/larmog/kubernetes-elk-cluster/blob/master/logstash-controller.yaml&#34;&gt;&lt;code&gt;logstash-controller.yaml&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Create logstash-ssl secret
$ conf2kube -n logstash-ssl -f &amp;lt;filename&amp;gt;.crt | kubectl create -f -
$ kubectl patch secret logstash-ssl -p `conf2kube -n logstash-ssl -f &amp;lt;filename&amp;gt;.key`
$
$ # Create logstash.conf secret
$ conf2kube -n logstash.conf -f logstash.conf | kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now have two &lt;code&gt;logstash&lt;/code&gt; secrets:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get secrets
NAME                        TYPE                                  DATA      AGE
default-token-i5v41         kubernetes.io/service-account-token   2         59d
logstash-ssl                Opaque                                2         1d
logstash.conf               Opaque                                1         1d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now clone the kubernetes-elk-cluster repo and create the logstash server and
kibana controller:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone https://github.com/larmog/kubernetes-elk-cluster.git
$ cd kubernetes-elk-cluster
$ kubectl create -f service-account.yaml
$ kubectl create -f logstash-service.yaml
$ kubectl create -f logstash-controller.yaml
$ kubectl create -f kibana-controller.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! You should now have &lt;code&gt;logstash&lt;/code&gt; and &lt;code&gt;kibana&lt;/code&gt; running!&lt;/p&gt;

&lt;p&gt;For you to be able to access &lt;code&gt;kibana&lt;/code&gt;, you need to expose the service outside
your cluster. I assume that you have used the &lt;code&gt;Kubernetes-On-ARM&lt;/code&gt; addon
&lt;code&gt;loadbalancer&lt;/code&gt;. Kibana doesn&amp;rsquo;t work with a sub-context so you need to expose
&lt;code&gt;kibana&lt;/code&gt; as a virtual host.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Run the following command with your Kibana host name.
$ sed -e &#39;s/kibana.kodbasen.org/&amp;lt;hostname&amp;gt;/g&#39; kibana-service.yaml &amp;gt; modified-kibana-service.yaml
$ kubectl create -f modified-kibana-service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If everything has gone according to plan you should see something similar:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl -I http://&amp;lt;hostname&amp;gt;
HTTP/1.1 400 Bad Request
kbn-name: kibana
kbn-version: 4.4.0
content-type: application/json; charset=utf-8
cache-control: no-cache
Date: Sat, 12 Mar 2016 11:16:37 GMT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Never mind the &lt;code&gt;400&lt;/code&gt; answer, you can see that Kibana has replied with
&lt;code&gt;kbn-name: kibana&lt;/code&gt; and &lt;code&gt;kbn-version: 4.4.0&lt;/code&gt;. Now try to open Kibana in your
favorite browser.&lt;/p&gt;

&lt;p&gt;We have covered a lot of ground, and you should give your self a tap on the
shoulder. But were not done yet. We have &lt;em&gt;elasticsearch&lt;/em&gt; running and &lt;em&gt;logstash&lt;/em&gt;
waiting for logs to be stored in elastic. &lt;em&gt;Kibana&lt;/em&gt; is hanging around waiting for
logs to appear in the &lt;code&gt;.kibana&lt;/code&gt; index. But we need to start thinking of how
to forward logs from our nodes to &lt;em&gt;logstash&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&#34;logstash-forwarder:5ef3c2c0e23696371af99bf964485763&#34;&gt;Logstash Forwarder&lt;/h4&gt;

&lt;p&gt;In this first part we will use &lt;code&gt;logstash-forwarder&lt;/code&gt; to collect logs from our
nodes and forward them to logstash. In the following parts of this series we
will look into other ways. The &lt;code&gt;logstash-forwarder&lt;/code&gt; project has been replaced
by &lt;code&gt;filebeat&lt;/code&gt; but that´s a topic for the next post.&lt;/p&gt;

&lt;p&gt;We need to run &lt;code&gt;logstash-forwarder&lt;/code&gt; on every node that we wan&amp;rsquo;t to collect logs
from. Kubernetes &lt;code&gt;1.2&lt;/code&gt; will include daemon sets, which is a great feature for
running things on a set of nodes but we are still running &lt;code&gt;1.1.x&lt;/code&gt;. In the
meantime, until &lt;code&gt;1.2&lt;/code&gt; is released, we&amp;rsquo;ll use static pods.&lt;/p&gt;

&lt;p&gt;Repeat the following steps on every node where you wan&amp;rsquo;t to collect logs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ mkdir -p /etc/logstash-forvarder/certs &amp;amp;&amp;amp; mkdir /etc/logstash-forvarder/config
$
$ # Copy the CA certificate that was used to create the logstash cert.
$ cp ca.crt /etc/logstash-forvarder/certs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the &lt;code&gt;logstash-forwarder.conf&lt;/code&gt; in &lt;code&gt;/etc/logstash-forvarder/config&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;network&amp;quot;: {
    &amp;quot;servers&amp;quot;: [
        &amp;quot;logstash.default.svc.cluster.local:5043&amp;quot;
    ],
    &amp;quot;timeout&amp;quot;: 15,
    &amp;quot;ssl ca&amp;quot;: &amp;quot;/logstash-forwarder/certs/ca.crt&amp;quot;
  },
  &amp;quot;files&amp;quot;: [
    {
      &amp;quot;paths&amp;quot;: [
        &amp;quot;/var/log/syslog&amp;quot;
      ],
      &amp;quot;fields&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;syslog&amp;quot;}
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the &lt;code&gt;logstash-forwarder&lt;/code&gt; static pod file &lt;code&gt;logstash-forwarder.json&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
  &amp;quot;kind&amp;quot;: &amp;quot;Pod&amp;quot;,
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;name&amp;quot;:&amp;quot;logstash-forwarder&amp;quot;
  },
  &amp;quot;spec&amp;quot;:{
    &amp;quot;containers&amp;quot;:[
      {
        &amp;quot;name&amp;quot;: &amp;quot;logstash-forwarder&amp;quot;,
        &amp;quot;image&amp;quot;: &amp;quot;larmog/logstash-forwarder:2.2.0&amp;quot;,
        &amp;quot;imagePullPolicy&amp;quot;: &amp;quot;IfNotPresent&amp;quot;,
        &amp;quot;command&amp;quot;: [
        ],
        &amp;quot;securityContext&amp;quot;: {
          &amp;quot;privileged&amp;quot;: true
        },
        &amp;quot;volumeMounts&amp;quot;:[{
          &amp;quot;name&amp;quot;: &amp;quot;certs&amp;quot;,
          &amp;quot;mountPath&amp;quot;: &amp;quot;/logstash-forwarder/certs&amp;quot;
        },
        {
          &amp;quot;name&amp;quot;: &amp;quot;config&amp;quot;,
          &amp;quot;mountPath&amp;quot;: &amp;quot;/logstash-forwarder/config&amp;quot;
        },
        {
          &amp;quot;name&amp;quot;: &amp;quot;syslog&amp;quot;,
          &amp;quot;mountPath&amp;quot;: &amp;quot;/var/log/syslog&amp;quot;
        }]
    }],
    &amp;quot;volumes&amp;quot;: [
      {
        &amp;quot;name&amp;quot;: &amp;quot;certs&amp;quot;,
        &amp;quot;hostPath&amp;quot;: {
          &amp;quot;path&amp;quot;: &amp;quot;/etc/logstash-forwarder/certs&amp;quot;
        }
      },{
        &amp;quot;name&amp;quot;: &amp;quot;config&amp;quot;,
        &amp;quot;hostPath&amp;quot;: {
          &amp;quot;path&amp;quot;: &amp;quot;/etc/logstash-forwarder/config&amp;quot;
        }
      },
      {
        &amp;quot;name&amp;quot;: &amp;quot;syslog&amp;quot;,
        &amp;quot;hostPath&amp;quot;: {
          &amp;quot;path&amp;quot;: &amp;quot;/var/log/syslog&amp;quot;
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Copy the file to the manifest directory. In Kubernetes-On-ARM there located
here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;On the master: &lt;code&gt;/etc/kubernetes/static-pods/master&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;On workers: &lt;code&gt;/etc/kubernetes/static-pods/worker&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;kubelet&lt;/code&gt; will pick up the static pod definition as soon as the file is
copied to the manifets directory. Run &lt;code&gt;docker ps|grep logstash-forwarder:2.2.0&lt;/code&gt;
on the node, and you should see the container running.&lt;/p&gt;

&lt;p&gt;And we are done! The &lt;code&gt;logstash-forwarder&lt;/code&gt; starts processing events and if you
look at the container log, you should see something similar:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker logs -f 8c4a3ba3f3e9
...
2016/03/11 15:13:41.575130 Registrar: processing 1024 events
2016/03/11 15:13:55.862568 Registrar: processing 1024 events
2016/03/11 15:14:01.671923 Registrar: processing 1024 events
2016/03/11 15:14:02.889663 Registrar: processing 256 events
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;to-sum-up:5ef3c2c0e23696371af99bf964485763&#34;&gt;To sum up&lt;/h4&gt;

&lt;p&gt;We have created a &lt;code&gt;elasticsearch&lt;/code&gt; cluster for storing logs, and &lt;code&gt;logstash&lt;/code&gt;for
processing incoming logs. We&amp;rsquo;ve also setup &lt;code&gt;kibana&lt;/code&gt; for searching and
visualizing. On our nodes we have installed &lt;code&gt;logstash-forwarder&lt;/code&gt;, to send all
&lt;code&gt;/var/log/syslog&lt;/code&gt; events to &lt;code&gt;logstash&lt;/code&gt;, using the &lt;code&gt;lumberjack&lt;/code&gt; protocol.&lt;/p&gt;

&lt;p&gt;In the next part in this series we will investigate how we can forward stdout
and stderr from our containers to &lt;code&gt;logstash&lt;/code&gt;. As always, I hope you picked up
something new and found it worth while reading.&lt;/p&gt;

&lt;h5 id=&#34;docker-images:5ef3c2c0e23696371af99bf964485763&#34;&gt;Docker images&lt;/h5&gt;

&lt;p&gt;All images are built from Pauls with generated &lt;code&gt;Dockerfile&lt;/code&gt;:s for &lt;code&gt;armhf&lt;/code&gt;.
The &lt;em&gt;Java&lt;/em&gt; base image for &lt;code&gt;elasticsearch&lt;/code&gt; is
&lt;a href=&#34;https://hub.docker.com/r/larmog/armhf-alpine-java/&#34;&gt;larmog/armhf-alpine-java&lt;/a&gt;&lt;/p&gt;

&lt;h5 id=&#34;acknowledgement:5ef3c2c0e23696371af99bf964485763&#34;&gt;Acknowledgement&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pires&#34;&gt;Paulo Pires&lt;/a&gt; repositories for
&lt;code&gt;elasticsearch&lt;/code&gt;, &lt;code&gt;logstash&lt;/code&gt; and &lt;code&gt;kibana&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/leannenorthrop&#34;&gt;Leanne Northrop&lt;/a&gt; repository for Docker
Java image on &lt;code&gt;armhf&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;links:5ef3c2c0e23696371af99bf964485763&#34;&gt;Links&lt;/h5&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pires/kubernetes-elasticsearch-cluster&#34;&gt;https://github.com/pires/kubernetes-elasticsearch-cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pires/kubernetes-elk-cluster&#34;&gt;https://github.com/pires/kubernetes-elk-cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://technologyconversations.com/2015/05/18/centralized-system-and-docker-logging-with-elk-stack/&#34;&gt;http://technologyconversations.com/2015/05/18/centralized-system-and-docker-logging-with-elk-stack/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://thepracticalsysadmin.com/shipping-logs-to-elk/&#34;&gt;https://thepracticalsysadmin.com/shipping-logs-to-elk/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.dasblinkenlichten.com/logging-in-kubernetes-with-fluentd-and-elasticsearch/&#34;&gt;http://www.dasblinkenlichten.com/logging-in-kubernetes-with-fluentd-and-elasticsearch/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://kubernetes.io/v1.1/docs/getting-started-guides/logging-elasticsearch.html&#34;&gt;http://kubernetes.io/v1.1/docs/getting-started-guides/logging-elasticsearch.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://kubernetes.io/v1.1/docs/user-guide/logging.html&#34;&gt;http://kubernetes.io/v1.1/docs/user-guide/logging.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.fluentd.org/guides/recipes/docker-logging&#34;&gt;http://www.fluentd.org/guides/recipes/docker-logging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch/fluentd-es-image&#34;&gt;https://github.com/GoogleCloudPlatform/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch/fluentd-es-image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gliderlabs/logspout&#34;&gt;https://github.com/gliderlabs/logspout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/looplab/logspout-logstash&#34;&gt;https://github.com/looplab/logspout-logstash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hub.docker.com/r/gliderlabs/logspout/~/dockerfile/&#34;&gt;https://hub.docker.com/r/gliderlabs/logspout/~/dockerfile/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://groups.google.com/forum/#!topic/google-containers/ZJzUIn_r3w4&#34;&gt;https://groups.google.com/forum/#!topic/google-containers/ZJzUIn_r3w4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;old &lt;a href=&#34;https://github.com/stuart-warren/logstash-input-journald&#34;&gt;https://github.com/stuart-warren/logstash-input-journald&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/logstash-plugins/logstash-input-journald&#34;&gt;https://github.com/logstash-plugins/logstash-input-journald&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/elastic/logstash/issues/1729&#34;&gt;https://github.com/elastic/logstash/issues/1729&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vaijab/logstash-filter-kubernetes&#34;&gt;https://github.com/vaijab/logstash-filter-kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.fluentd.org&#34;&gt;http://www.fluentd.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Nothing for the faint hearted</title>
      <link>http://larmog.github.io/2016/03/03/nothing-for-the-faint-hearted/</link>
      <pubDate>Thu, 03 Mar 2016 09:12:49 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/03/03/nothing-for-the-faint-hearted/</guid>
      <description>

&lt;p&gt;I’ve been running my &lt;em&gt;Kubernetes-On-ARM&lt;/em&gt; cluster a couple of weeks now and I’ll
try to summarize my experiences. If you followed my earlier posts, then you know
that I&amp;rsquo;m running &lt;a href=&#34;https://gogs.io/&#34;&gt;Gogs&lt;/a&gt; and
&lt;a href=&#34;https://github.com/drone&#34;&gt;Drone&lt;/a&gt; on a Kubernetes cluster with
&lt;a href=&#34;https://www.gluster.org/&#34;&gt;GlusterFS&lt;/a&gt; for storing. I’m using &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes-On-ARM&lt;/a&gt;
on &lt;a href=&#34;http://blog.hypriot.com/&#34;&gt;HypriotOS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post I will try to summarize my experiences so far.&lt;/p&gt;

&lt;h5 id=&#34;dind-and-kubelet-does-not-play-well-together:38dbe607a3f0a33b702c4d61c17d021b&#34;&gt;Dind and Kubelet does not play well together&lt;/h5&gt;

&lt;p&gt;One of the reasons that I wanted to run Drone was it’s use of Docker, and to
build and push docker images to a registry. Drone has a plugin that does all
that and uses &lt;code&gt;dind&lt;/code&gt; (Docker in Docker).&lt;/p&gt;

&lt;p&gt;The examples and &lt;a href=&#34;https://github.com/drone-plugins/drone-docker/blob/master/DOCS.md&#34;&gt;documentation&lt;/a&gt;
is rather straightforward, nothing complicated. You need to install and setup
the &lt;a href=&#34;http://readme.drone.io/devs/cli/&#34;&gt;client&lt;/a&gt;, and setup your
&lt;a href=&#34;http://readme.drone.io/usage/secrets/&#34;&gt;secrets&lt;/a&gt; so that you don’t store your
username and password in your build file.&lt;/p&gt;

&lt;p&gt;There was only one problem, it didn&amp;rsquo;t work …&lt;/p&gt;

&lt;p&gt;I could not get the plugin to work. After turning on &lt;code&gt;DEBUG&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# .drone.yml
...
publish:
  docker:
    environment:
    - DOCKER_LAUNCH_DEBUG=true
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I could see this error message:

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Drone Docker plugin DEBUG&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/drone-log.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;It gave me a real headache and finally, after some googling, I found these
issues:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/drone/drone/issues/1352&#34;&gt;https://github.com/drone/drone/issues/1352&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/20671&#34;&gt;https://github.com/kubernetes/kubernetes/issues/20671&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/18202&#34;&gt;https://github.com/kubernetes/kubernetes/issues/18202&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;kubelet&lt;/code&gt; is causing the &lt;code&gt;/sys/fs/docker&lt;/code&gt; file-system to becoming read-only,
this will hopefully be resolved in Kubernetes 1.2. Meantime, the solution is,
to remove one of the nodes from the Kubernetes cluster and use it as a remote
worker-node in Drone.&lt;/p&gt;

&lt;h5 id=&#34;containerized-kubelet:38dbe607a3f0a33b702c4d61c17d021b&#34;&gt;Containerized Kubelet&lt;/h5&gt;

&lt;p&gt;There are issues, when running the &lt;code&gt;kubelet&lt;/code&gt; in a container, and is work in
progress, to resolve those issues.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/4869&#34;&gt;https://github.com/kubernetes/kubernetes/issues/4869&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/14403&#34;&gt;https://github.com/kubernetes/kubernetes/issues/14403&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;glusterfs-usb-and-100mbit-ethernet-network:38dbe607a3f0a33b702c4d61c17d021b&#34;&gt;Glusterfs USB and 100Mbit/Ethernet network&lt;/h5&gt;

&lt;p&gt;I’ve been wanting to try out &lt;a href=&#34;http://go.cd&#34;&gt;Go CD&lt;/a&gt; for some time now, and try to set up a pipeline on Kubernetes. If you take a look at the &lt;a href=&#34;https://docs.go.cd/current/installation/system_requirements.html&#34;&gt;system requirements&lt;/a&gt; for Go CD, you will see that this is not suitable for a Raspberry Pi. The Go Server is a giant monolith, but what the heck, sometimes you need to check out the limit. I created a Docker image for ARM and tried to start up a pod.&lt;/p&gt;

&lt;p&gt;I got the &lt;em&gt;GO CD Server&lt;/em&gt; started, but it took an hour. Most of the cpu-time during
start was spent on &lt;code&gt;glusterfs&lt;/code&gt;. The &lt;em&gt;GO CD Server&lt;/em&gt; uses a lot of I/O during
start and I measured the read/write speed on the glusterfs to around 1,5-2 MB/s.
One should not expect any great performance when running &lt;code&gt;glusterfs&lt;/code&gt; on
&lt;em&gt;Raspberry Pi&lt;/em&gt;. There are some real bottlenecks on network and USB.&lt;/p&gt;

&lt;h5 id=&#34;upgrading-to-docker-v1-10-2:38dbe607a3f0a33b702c4d61c17d021b&#34;&gt;Upgrading to Docker v1.10.2&lt;/h5&gt;

&lt;p&gt;The &lt;em&gt;Hypriot&lt;/em&gt; team has written a post about &lt;a href=&#34;http://blog.hypriot.com/post/test-build-and-package-docker-for-arm-the-official-way/&#34;&gt;Test, build and package Docker for ARM the official way&lt;/a&gt;.
That was one of reasons why I wanted to upgrade Docker to &lt;code&gt;v1.10.2&lt;/code&gt;.
But has shown to have some issues. The &lt;code&gt;kubectl exec&lt;/code&gt; command doesn’t work
anymore and is reported here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/20884&#34;&gt;https://github.com/kubernetes/kubernetes/issues/20884&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;and is a duplicate of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/19720&#34;&gt;https://github.com/kubernetes/kubernetes/issues/19720&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fsouza/go-dockerclient/issues/455&#34;&gt;https://github.com/fsouza/go-dockerclient/issues/455&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;summarize:38dbe607a3f0a33b702c4d61c17d021b&#34;&gt;Summarize&lt;/h5&gt;

&lt;p&gt;Running &lt;em&gt;Kubernetes-On-ARM&lt;/em&gt; together with new releases of &lt;em&gt;Docker&lt;/em&gt; is nothing
for the faint hearted. There are some issues that hopefully will be resolved in
a near future. I’m still very pleased with the cluster and it’s a great
environment for building &lt;em&gt;microservices&lt;/em&gt; and validating your system
architecture. For instance, it’s not possible to build a big monolith.
If your system runs and scales on Kubernetes-On-ARM (built on Raspberry Pi)
then you can be sure of that it runs and scales on any platform.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GlusterFS On Kubernetes ARM</title>
      <link>http://larmog.github.io/2016/02/22/glusterfs-on-kubernetes-arm/</link>
      <pubDate>Mon, 22 Feb 2016 09:35:30 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/22/glusterfs-on-kubernetes-arm/</guid>
      <description>&lt;p&gt;If you followed my earlier posts, you know that I’m running a Kubernetes cluster
on Raspberry Pi, using &lt;strong&gt;HypriotOS&lt;/strong&gt; and Lucas Käldströms &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes-On-ARM&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;If not you can find my earlier posts here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;Kubernetes-On-ARM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/&#34;&gt;Gogs and Drone On Kubernetes-ARM - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/&#34;&gt;Gogs and Drone On Kubernetes-ARM - Part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-3/&#34;&gt;Gogs and Drone On Kubernetes-ARM - Part 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;ve used &lt;strong&gt;NFS&lt;/strong&gt; volumes for storage and DiskStation NAS as NFS server.
The solution have had some drawbacks and it&amp;rsquo;s been hard to synchronize users and
groups, and I&amp;rsquo;ve used &lt;code&gt;all_squash&lt;/code&gt; to a specific &lt;code&gt;uid&lt;/code&gt;. That worked for &lt;strong&gt;Gogs&lt;/strong&gt;
and &lt;strong&gt;Drone&lt;/strong&gt; but not for &lt;strong&gt;MySQL&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Another alternative is to use &lt;strong&gt;GlusterFS&lt;/strong&gt; volumes. GlusterFS is a clustered
file-system that is capable of scaling to several peta-bytes.
GlusterFS aggregates storage bricks, that can be made of commodity hardware.
When I started this project, I had two
&lt;code&gt;Raspberry Pi 1 B+&lt;/code&gt; in my desk drawer. I&amp;rsquo;ve used them as nodes in the cluster,
but the small amount of &lt;code&gt;RAM&lt;/code&gt; has shown them less useful compared to the newer
&lt;code&gt;Raspberry Pi 2 B&lt;/code&gt; based nodes. But then it hit me that maybe I could use the
old Pi 1 boards as bricks in GlusterFS. One of the purposes with this project is
to see how much of a Kubernetes cluster you can get for a reasonable amount. So
I&amp;rsquo;d like to keep a tight budget. Normally when setting up a brick you want to
use RAID, because you don&amp;rsquo;t want to handle disk failures on a brick level, but
for this project it&amp;rsquo;s a reasonable solution.&lt;/p&gt;

&lt;p&gt;After a trip to my local Electronic Retailers I came home with this
(not the cluster):

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;GlusterFS ingredients&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/IMG_1953.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;* 2 WD external usb harddrives&lt;/li&gt;
&lt;li&gt;* 2 Raspberry Pi 2 B boards (to replace the Pi 1:s)&lt;/li&gt;
&lt;li&gt;* 2 SD cards&lt;/li&gt;
&lt;li&gt;* One more Multi-Pi Stackable Raspberry Pi Case&lt;/li&gt;
&lt;li&gt;* Cables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My first attempt was to connect the external drives to the usb port. That didn&amp;rsquo;t
work of course, external disks needs more power than what the Pi usb port can
deliver. There are two alternatives for solving this. You could use an external
usb-hub with it&amp;rsquo;s own power supply. But that means two usb-hubs and two power
adapters. The other alternative is to use a usb Y-cable and connect the extra
connector to the usb supercharger (that powers all nodes). After one more trip
to the store to get a Y-cable and then connect it to the usb supercharger, the
problem was solved.&lt;/p&gt;

&lt;p&gt;The image shows the cluster after assembling.

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Cluster ready for GlusterFS&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/IMG_1961.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s room for two more Pi:s in the switch and usb supercharger.&lt;/p&gt;

&lt;p&gt;Next step was to partition and format the disks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Prepare your disk and create a partition first
$ # Install xfs tools
$ apt-get install -y xfsprogs
$ # Format the disk
$ mkfs.xfs -f -L brick1 -i size=512 /dev/sda1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and mount and install &lt;code&gt;glusterfs&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Create and mount the brick
$ mkdir -p /data/brick1
$ echo &#39;/dev/sda1 /data/brick1 xfs defaults 1 2&#39; &amp;gt;&amp;gt; /etc/fstab
$ mount -a &amp;amp;&amp;amp; mount
# Install GlusterFS server
$ apt-get install -y glusterfs-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The steps above was made on both servers.&lt;/p&gt;

&lt;p&gt;Next we needed to connect our storage servers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ #From server1
$ gluster peer probe server2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ #From server2
$ gluster peer probe server1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Time to create the first volume:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ mkdir /data/brick1/vol0
$ gluster volume create vol0 replica 2 store1.local:/data/brick1/vol0 store2.local:/data/brick2/vol0
$ gluster volume start vol0
$ gluster volume info

Volume Name: vol0
Type: Replicate
Volume ID: 6ffe7723-e0ac-44c7-a004-fa951467884d
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: server1:/data/brick1/vol0
Brick2: server2:/data/brick2/vol0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next you need to install &lt;code&gt;glusterfs-client&lt;/code&gt; on all your Kubernetes nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ apt-get install -y glusterfs-client
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done.&lt;/p&gt;

&lt;p&gt;The only thing left is to set up our &lt;code&gt;glusterfs-cluster endpoints&lt;/code&gt; as described
in the &lt;a href=&#34;http://kubernetes.io/v1.1/examples/glusterfs/README.html&#34;&gt;Kubernetes documentation for GlusterFS Volumes&lt;/a&gt; and configure
our &lt;a href=&#34;http://kubernetes.io/v1.1/docs/user-guide/persistent-volumes.html&#34;&gt;Persistent Volumes and Claims&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Migrating the volumes from NFS to GlusterFS was very easy. The only problem
I&amp;rsquo;ve had was a silly mistake. Coming from NFS I first tried to share a volume
between applications, using different paths: &lt;code&gt;/vol 0/mysql&lt;/code&gt;, &lt;code&gt;/vol/drone&lt;/code&gt; that
didn&amp;rsquo;t work. You need to create different volumes for each application. Except
from the small misconception, there&amp;rsquo;s been no problems at all.&lt;/p&gt;

&lt;p&gt;Now finally I have a Kubernetes cluster running on bare metal that can be used
for trying out techniques, for building microservices.&lt;/p&gt;

&lt;p&gt;By the way, I took the opportunity to overclock my Raspberry Pi:s.
Thanks to &lt;a href=&#34;http://haydenjames.io/raspberry-pi-2-overclock/&#34;&gt;Hayden James&lt;/a&gt;, all
cluster nodes now runs at 1000 Mhz, and it seems stable.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dashboard on Kubernetes ARM</title>
      <link>http://larmog.github.io/2016/02/13/dashboard-on-kubernetes-arm/</link>
      <pubDate>Sat, 13 Feb 2016 11:50:12 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/13/dashboard-on-kubernetes-arm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas Käldström&lt;/a&gt; added the new &lt;strong&gt;Dashboard&lt;/strong&gt; addon
in the &lt;code&gt;dev&lt;/code&gt; bransch to &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes On ARM&lt;/a&gt;
and I decided to try it out.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how to install it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ mkdir /etc/kubernetes/source/addons/dashboard
$ curl -sSL https://raw.githubusercontent.com/luxas/kubernetes-on-arm/dev/addons/dashboard.yaml &amp;gt; \ /etc/kubernetes/source/addons/dashboarddashboard.yaml
$ kube-config enable-addon dashboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here it is:

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Kubernetes Dashboard on ARM&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/dashboard.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;Cool, Thanks &lt;a href=&#34;https://twitter.com/kubernetesonarm&#34;&gt;Lucas&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gogs and Drone On Kubernetes-ARM - Part 3</title>
      <link>http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-3/</link>
      <pubDate>Mon, 08 Feb 2016 16:39:34 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-3/</guid>
      <description>

&lt;p&gt;This is the third part in a series on setting up Gogs and Drone on
Kubernetes-ARM. You can find the earlier posts here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;Kubernetes-On-ARM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/&#34;&gt;Gogs and Drone &amp;hellip; Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/&#34;&gt;Gogs and Drone &amp;hellip; Part 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this third episode I will try to show how to setup Drone on Kubernetes-ARM.
A disclaimer, I&amp;rsquo;m new to Drone and out on deep water so&amp;hellip;&lt;/p&gt;

&lt;p&gt;If your wondering about Drone take a look at
&lt;a href=&#34;https://github.com/drone/drone&#34;&gt;GitHub page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;ldquo;Every build is executed inside an ephemeral Docker container, giving
developers complete control over their build environment with guaranteed
isolation.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This means that the &lt;code&gt;build agent&lt;/code&gt; is a container and all the plugins are also
containers. This makes it a bit problematic running Drone on ARM because we
need to rebuild everything, from Drone it self to all plugins that we wan&amp;rsquo;t to
use. We need to setup a Drone infrastructure for ARM, &lt;strong&gt;or so I thought&amp;hellip;&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&#34;on-the-shoulder-of-others:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;On the shoulder of others&lt;/h5&gt;

&lt;p&gt;Fortunately before I jumped in to the &lt;em&gt;rabbit hole&lt;/em&gt;, not knowing if it was a
dead end, I found two different initiatives for running Drone on ARM. You can
read more about it here:
&lt;a href=&#34;https://discuss.drone.io/t/drone-ported-to-arm/55&#34;&gt;Drone ported to ARM&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I decided to try
&lt;a href=&#34;https://github.com/armhf-docker-library/drone&#34;&gt;armhf-docker-library/drone&lt;/a&gt; and
I also found this example from Greg Taylor: &lt;a href=&#34;https://github.com/drone-demos/drone-on-kubernetes/&#34;&gt;drone-on-kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;h5 id=&#34;lets-get-started:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;Lets get started&lt;/h5&gt;

&lt;p&gt;First we need to create persistent volume for our sqlite database:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-drone
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: &amp;lt;Path exported NFS volume&amp;gt;
    server: &amp;lt;IP NFS Server&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and a volume claim:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-drone
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now create our replication controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ReplicationController
metadata:
  labels:
    app: droneio
  name: droneio
  namespace: default
spec:
  replicas: 1
  selector:
    app: droneio
  template:
    metadata:
      labels:
        app: droneio
    spec:
      containers:
      # Drone ARM port https://github.com/armhf-docker-library
      - image: armhfbuild/drone:latest
        imagePullPolicy: Always
        name: droneio
        ports:
          - containerPort: 8000
            name: web   
            protocol: TCP
        env:
        - name: REMOTE_DRIVER
          value: &amp;quot;gogs&amp;quot;
        - name: REMOTE_CONFIG
          value: &amp;quot;https://&amp;lt;YOUR GOGS SERVICE&amp;gt;?skip_verify=true&amp;amp;open=false&amp;quot;
        - name: DEBUG
          value: &amp;quot;true&amp;quot;
        #Drone ARM port plugins
        - name: PLUGIN_FILTER
          value: &amp;quot;armhfplugins/*&amp;quot;
        volumeMounts:
          - mountPath: &amp;quot;/var/lib/drone&amp;quot;
            name: persistentdata
          - mountPath: /var/run/docker.sock
            name: docker-socket
          - mountPath: /var/lib/docker
            name: docker-lib
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      volumes:
          - name: persistentdata
            persistentVolumeClaim:
              claimName: pvc-drone
          - name: docker-socket
            hostPath:
              path: /var/run/docker.sock
          - name: docker-lib
            hostPath:
              path: /var/lib/docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at our controllers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get rc
CONTROLLER   CONTAINER(S)   IMAGE(S)                        SELECTOR       REPLICAS   AGE
droneio      droneio        armhfbuild/drone:latest         app=droneio    1          2h
gogs         gogs           larmog/rpi-gogs:0.8.23.0126-2   app=gogs       1          10d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we create our service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  annotations:
    # For our service-loadbalancer
    serviceloadbalancer/lb.sslTerm: &amp;quot;true&amp;quot;
    serviceloadbalancer/lb.host: &amp;lt;virtual host&amp;gt;
    serviceloadbalancer/lb.cookie-sticky-session: &amp;quot;true&amp;quot;
  labels:
    app: droneio
  name: droneio
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8000
  selector:
    app: droneio
  sessionAffinity: None
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here´s our services:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get svc
NAME         CLUSTER_IP   EXTERNAL_IP   PORT(S)    SELECTOR       AGE
droneio      10.0.0.11    &amp;lt;none&amp;gt;        80/TCP     app=droneio    2h
gogs         10.0.0.85    &amp;lt;none&amp;gt;        80/TCP     app=gogs       13d
gogs-ssh     10.0.0.216   nodes         2222/TCP   app=gogs       13d
kubernetes   10.0.0.1     &amp;lt;none&amp;gt;        443/TCP    &amp;lt;none&amp;gt;         29d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If all has gone according to our plan we should see the service in our
&lt;code&gt;ha-proxy&lt;/code&gt; status page, and you should be able to login with your &lt;em&gt;Gogs&lt;/em&gt;
account.&lt;/p&gt;

&lt;h5 id=&#34;ca-and-pki-infrastructure:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;CA and PKI infrastructure&lt;/h5&gt;

&lt;p&gt;If your using your own CA and PKI infrastructure and a service-loadbalancer,
then there is a couple of things you need to do. When you have activated a
repository in Drone, you need to edit the Webhook that notifies Drone.
Use the service name or the IP address of your Drone service
(&lt;code&gt;droneio.&amp;lt;namespace&amp;gt;&lt;/code&gt;) in Kubernetes.

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Gogs Webhook&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/gogs-webhook.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;There is no easy way (at least none that i know of) to add your CA certificate.
This means that the &lt;code&gt;plugins/drone-git&lt;/code&gt; will fail to clone your repository using
&lt;code&gt;https&lt;/code&gt;. It gave me a serious headache until i found a solution and I admit it&amp;rsquo;s
a bit of a hack.

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;drone-git failed to clone&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/drone-git-failed.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;You need to set &lt;code&gt;GIT_SSL_NO_VERIFY=true&lt;/code&gt; in your &lt;code&gt;.drone.yml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clone:
    environment:
      - GIT_SSL_NO_VERIFY=true
    path: /xxxx
&lt;/code&gt;&lt;/pre&gt;


&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;drone-git cloned successfully&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/drone-git-success.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;



&lt;h5 id=&#34;conclusion:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;Conclusion&lt;/h5&gt;

&lt;p&gt;This concludes our series about running Gogs and Drone on Kubernetes-ARM.
Thanks to the excellent job done by others, it is a rather straight forward
process to set it up. I hope you liked the series and hope you&amp;rsquo;ve picked up
something you didn&amp;rsquo;t know before. Now I&amp;rsquo;m off to learn some more about &lt;em&gt;Drone&lt;/em&gt;.&lt;/p&gt;

&lt;h5 id=&#34;finally:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;Finally&lt;/h5&gt;

&lt;p&gt;Some useful links:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.hypriot.com/&#34;&gt;http://blog.hypriot.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;https://github.com/luxas/kubernetes-on-arm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jinkit.com/k8s-on-rpi/&#34;&gt;http://www.jinkit.com/k8s-on-rpi/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://discuss.drone.io/t/drone-ported-to-arm/55&#34;&gt;https://discuss.drone.io/t/drone-ported-to-arm/55&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kelseyhightower/docker-kubernetes-tls-guide&#34;&gt;https://github.com/kelseyhightower/docker-kubernetes-tls-guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kelseyhightower/conf2kube&#34;&gt;https://github.com/kelseyhightower/conf2kube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/drone-demos&#34;&gt;https://github.com/drone-demos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Gogs and Drone On Kubernetes-ARM - Part 2</title>
      <link>http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/</link>
      <pubDate>Mon, 08 Feb 2016 09:21:47 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/</guid>
      <description>&lt;p&gt;This is the second part in a series of posts describing how I have setup Gogs
and Drone on
&lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;Kubernetes-On-ARM&lt;/a&gt; cluster.
In &lt;a href=&#34;http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/&#34;&gt;Part 1&lt;/a&gt; we
talked about setting up Gogs.&lt;/p&gt;

&lt;p&gt;In this part I&amp;rsquo;ll explain how to setup &lt;code&gt;service-loadbalancer&lt;/code&gt; to expose services
outside the cluster.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m using &lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas Käldström&lt;/a&gt;:s great
&lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;kubernetes-on-arm&lt;/a&gt; project.
There&amp;rsquo;s a load-balancer addon based on &lt;a href=&#34;https://github.com/kubernetes/contrib&#34;&gt;kubernetes/contrib/service-loadbalancer&lt;/a&gt;
that wasn&amp;rsquo;t ready in the &lt;code&gt;0.6.3&lt;/code&gt; release.&lt;/p&gt;

&lt;p&gt;Before we begin you might take a look at
&lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/service-loadbalancer&#34;&gt;service-loadbalancer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What I did was to build the &lt;code&gt;service-loadbalancer&lt;/code&gt; from the
&lt;code&gt;kubernetes/contrib master branch&lt;/code&gt;. Once again Lucas have made a great job and
created a docker file for building the Kubernetes-ARM binaries on x86.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Clone the repository
$ git clone https://github.com/luxas/kubernetes-on-arm
$ # Build the Docker image
$ cd kubernetes-on-arm/scripts/build-k8s-on-amd64
$ docker build -t build-k8s-on-amd64 .
$ # Create a container
$ docker run --name=build-k8s-on-amd64 build-k8s-on-amd64 true
$ # Copy out the binaries from the container
$ docker cp build-k8s-on-amd64:/output .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;service-loadbalancer&lt;/code&gt; binary is located in the &lt;code&gt;output&lt;/code&gt; directory.
Transfer the file to the nodes that will have the role of load-balancer.
e.g.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ scp ./output/service-loadbalancer \ xxx.xxx.xxx.xxx:/etc/kubernetes/source/images/kubernetesonarm/_bin/latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;service-loadbalancer&lt;/code&gt; uses a template for creating a &lt;code&gt;ha-proxy.cfg&lt;/code&gt;.
The template I&amp;rsquo;m using can be found here: &lt;a href=&#34;https://goo.gl/TzvKhX&#34;&gt;template.cfg&lt;/a&gt;
On each node build the &lt;code&gt;kubernetesonarm/loadbalancer&lt;/code&gt; image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd /etc/kubernetes/source/images/kubernetesonarm/loadbalancer
$ mv template.cfg template.cfg.org
$ wget wget https://goo.gl/TzvKhX -O template.cfg
$ ./build.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phew&amp;hellip; were half way. The Docker image is in place on our nodes. Now we need
to label them so that the scheduler can place the &lt;code&gt;service-loadbalancer&lt;/code&gt; on the
right nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl label --overwrite nodes &amp;lt;node name&amp;gt; role=loadbalancer
$ kubectl get nodes
NAME           LABELS                                                  STATUS    AGE
&amp;lt;node name&amp;gt;   kubernetes.io/hostname=&amp;lt;node name&amp;gt;,role=loadbalancer     Ready     2d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it&amp;rsquo;s time to create the &lt;code&gt;loadbalancer&lt;/code&gt;. If you wan&amp;rsquo;t to use &lt;code&gt;https&lt;/code&gt; you need
to create a &lt;code&gt;secret&lt;/code&gt; and mount the volume in your pod template.
Here&amp;rsquo;s my &lt;code&gt;loadbalancer-rc.yaml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ReplicationController
metadata:
  name: service-loadbalancer
  namespace: kube-system
  labels:
    app: service-loadbalancer
    version: v1
spec:
  replicas: 1
  selector:
    app: service-loadbalancer
    version: v1
  template:
    metadata:
      labels:
        app: service-loadbalancer
        version: v1
    spec:
      nodeSelector:
        role: loadbalancer
      volumes:
      - name: ssl-volume
        secret:
          secretName: kodbasen-ssl-secret
      containers:
      - image: kubernetesonarm/loadbalancer
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        name: haproxy
        ports:
        # All http services
        - containerPort: 80
          hostPort: 80
          protocol: TCP
        # nginx https
        - containerPort: 443
          hostPort: 443
          protocol: TCP
        # mysql
        - containerPort: 3306
          hostPort: 3306
          protocol: TCP
        # haproxy stats
        - containerPort: 1936
          hostPort: 1936
          protocol: TCP
        # gogs ssh
        - containerPort: 2222
          hostPort: 2222
          protocol: TCP
        volumeMounts:
        - name: ssl-volume
          readOnly: true
          mountPath: &amp;quot;/ssl&amp;quot;
        resources: {}
        args:
        - --tcp-services=my-gogs-ssh:2222
        - --ssl-cert=/ssl/server.pem
        - --ssl-ca-cert=/ssl/ca.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;He&amp;rsquo;re you can see that I&amp;rsquo;ve exposed the &lt;code&gt;my-gogs-ssh&lt;/code&gt; as a TCP service.&lt;/p&gt;

&lt;p&gt;Now we&amp;rsquo;re ready to expose our Gogs service to the outside world. We need to
change our Gogs service from
&lt;a href=&#34;http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/&#34;&gt;Part 1&lt;/a&gt; slightly and add
some annotations.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  annotations:
    serviceloadbalancer/lb.sslTerm: &amp;quot;true&amp;quot;
    serviceloadbalancer/lb.host: &amp;quot;gogs.replace.me&amp;quot;
    serviceloadbalancer/lb.cookie-sticky-session: &amp;quot;true&amp;quot;
  labels:
    app: my-gogs-service
  name: my-gogs-service
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 3000
  selector:
    app: my-gogs
  sessionAffinity: None
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;serviceloadbalancer/lb.sslTerm: &amp;quot;true&amp;quot;&lt;/code&gt; annotation says that we wan&amp;rsquo;t to
use &lt;code&gt;https&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serviceloadbalancer/lb.host: &amp;quot;gogs.replace.me&amp;quot;&lt;/code&gt; is the &lt;code&gt;virtual host&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serviceloadbalancer/lb.cookie-sticky-session: &amp;quot;true&amp;quot;&lt;/code&gt; enables sticky sessions
between your pods (&lt;code&gt;replicas &amp;gt; 1&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If everything is working you should be rewarded with a &lt;code&gt;ha-proxy&lt;/code&gt; status page
where you can monitor your exposed services. Fire up
&lt;code&gt;http://&amp;lt;loadbalancer ip&amp;gt;:1936/&lt;/code&gt; in your favorite browser and take a look.&lt;/p&gt;

&lt;p&gt;That is all for now. In the next part we will take a look at the &lt;code&gt;CI&lt;/code&gt; tool
&lt;a href=&#34;https://github.com/drone/drone&#34;&gt;Drone&lt;/a&gt; and how to get it working on our
&lt;em&gt;Kubernetes-ARM&lt;/em&gt; cluster. Prepare for a journey &lt;em&gt;down the rabbit hole&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gogs and Drone On Kubernetes-ARM - Part 1</title>
      <link>http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/</link>
      <pubDate>Sun, 07 Feb 2016 11:17:02 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/</guid>
      <description>&lt;p&gt;This is part 1 in a series of posts describing how I have setup Gogs and Drone
on my Kubernetes-ARM cluster. &lt;a href=&#34;https://gogs.io/&#34;&gt;Gogs - Go Git Service&lt;/a&gt; is
&lt;em&gt;A painless self-hosted Git service&lt;/em&gt; and is a great alternative when you can&amp;rsquo;t
use GitHub or wan&amp;rsquo;t to host your own Git service.&lt;/p&gt;

&lt;p&gt;The easiest way to get started with Gogs (and of course the only alternative if
you wan&amp;rsquo;t to use Kubernetes) is to use a Docker image. Gogs has a Docker image
ready on Docker Hub. Unfortunately that image won&amp;rsquo;t work on ARM.
Thankfully the Hypriot team has two Gogs Docker images ready: &lt;a href=&#34;https://hub.docker.com/r/hypriot/rpi-gogs-raspbian/&#34;&gt;hypriot/rpi-gogs-raspbian&lt;/a&gt;
and &lt;a href=&#34;https://hub.docker.com/r/hypriot/rpi-gogs-alpine/&#34;&gt;hypriot/rpi-gogs-alpine&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lets try it out:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl run my-gogs --image=hypriot/rpi-gogs-alpine --replicas=1 --port=3000
replicationcontroller &amp;quot;my-gogs-service&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we can see our pod is running:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods
NAME                      READY     STATUS    RESTARTS   AGE
my-gogs-3adh8             1/1       Running   0          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it&amp;rsquo;s time to expose our new pod as a service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl expose rc my-gogs --port=80 --target-port=3000 --name=my-gogs-service
service &amp;quot;my-gogs-service&amp;quot; exposed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If all went well we can now access or new service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl -s http://[master-ip]:8080/api/v1/proxy/namespaces/default/services/my-gogs-service/install|grep Version:
&amp;lt;p class=&amp;quot;left&amp;quot; id=&amp;quot;footer-rights&amp;quot;&amp;gt;© 2015 Gogs · Version: 0.6.1.0325 Beta Page:&amp;lt;strong&amp;gt;2259ms&amp;lt;/strong&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But oops! that´s kind of an old version. Our goal is to use Gogs together with
Drone and this won&amp;rsquo;t work. We need a version greater than &lt;code&gt;0.6.16.1022&lt;/code&gt;
(&lt;a href=&#34;http://readme.drone.io/setup/gogs/&#34;&gt;see&lt;/a&gt;). I guess this is the difference
between &lt;em&gt;leading edge&lt;/em&gt; and &lt;em&gt;bleeding edge&lt;/em&gt;. Again we&amp;rsquo;re saved by some one else&amp;rsquo;s
work. Gogs has a &lt;code&gt;Dockerfile.rpi&lt;/code&gt; ready that we can use to build our own image.
I&amp;rsquo;ve built and pushed an image to Docker Hub that you can use:
&lt;a href=&#34;https://hub.docker.com/r/larmog/rpi-gogs/&#34;&gt;&lt;code&gt;larmog/rpi-gogs&lt;/code&gt;&lt;/a&gt; that is &lt;code&gt;33MB&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So lets repeat the steps above with the new image and &lt;code&gt;curl&lt;/code&gt; for the version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl -s http://[master-ip]:8080/api/v1/proxy/namespaces/default/services/my-gogs-service/install|grep Version
© 2016 Gogs Version: 0.8.23.0126 Page: &amp;lt;strong&amp;gt;1622ms&amp;lt;/strong&amp;gt; Template: &amp;lt;strong&amp;gt;1619ms&amp;lt;/strong&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Version &lt;code&gt;0.8.23.0126&lt;/code&gt;, that looks so much better don&amp;rsquo;t, you think?&lt;/p&gt;

&lt;p&gt;Next step is to install Gogs. But hey&amp;hellip; wait a minute - what about persistence?
We need to add a &lt;em&gt;Volume&lt;/em&gt;. I&amp;rsquo;m using my home NAS, a DiskStation, over NFS. The
only thing we need to do is to share a volume over NFS and install NFS on our
nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo apt-get -y install nfs-common
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we need to create a &lt;code&gt;PersistentVolume&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-gogs
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /volume1/kbn1/gogs
    server: my-nfs-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-gogs
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and lastly mount the volume in our pod template:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ReplicationController
metadata:
  labels:
    app: my-gogs
  name: my-gogs
  namespace: default
spec:
  replicas: 1
  selector:
    app: my-gogs
  template:
    metadata:
      labels:
        app: my-gogs
    spec:
      containers:
      - image: larmog/rpi-gogs:0.8.23.0126-2
        imagePullPolicy: IfNotPresent
        name: my-gogs
        volumeMounts:
        - mountPath: &amp;quot;/data&amp;quot;
          name: persistentdata
        resources: {}
        ports:
          - containerPort: 3000
            name: web   
            protocol: TCP
          - containerPort: 22
            name: ssh
            protocol: TCP
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      volumes:
        - name: persistentdata
          persistentVolumeClaim:
            claimName: pvc-gogs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NFS v4 is kind of hard to use if you don&amp;rsquo;t have synchronized your users and
groups in your domain. I use &lt;code&gt;all_squash&lt;/code&gt; to a specific UID/GID in order to get
it to work with my NAS, and that works fine for Gogs but I&amp;rsquo;ve got plans to
replace NFS with &lt;a href=&#34;https://www.gluster.org/&#34;&gt;GlusterFS&lt;/a&gt; and it&amp;rsquo;s on the &lt;code&gt;TODO&lt;/code&gt;
list.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s enjoy the fruit of our work (or as we say in Sweden: &amp;ldquo;ett Ernst ögonblick&amp;rdquo;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get services
NAME             CLUSTER_IP   EXTERNAL_IP   PORT(S)    SELECTOR       AGE
my-gogs-service  10.0.0.85    &amp;lt;none&amp;gt;        80/TCP     app=gogs       9d
my-gogs-ssh      10.0.0.216   nodes         2222/TCP   app=gogs       9d
kubernetes       10.0.0.1     &amp;lt;none&amp;gt;        443/TCP    &amp;lt;none&amp;gt;         25d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that I&amp;rsquo;ve also created a service for the &lt;code&gt;ssh&lt;/code&gt; port.
Now we can complete the Gogs installation. Open the url (http://[master-ip]:8080/api/v1/proxy/namespaces/default/services/my-gogs-service)
and complete the installation.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/&#34;&gt;Part 2&lt;/a&gt; I will
explain how to set up &lt;code&gt;service-loadbalancer&lt;/code&gt; to expose your services outside your
cluster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes On ARM</title>
      <link>http://larmog.github.io/2016/02/06/kubernetes-on-arm/</link>
      <pubDate>Sat, 06 Feb 2016 10:33:08 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/06/kubernetes-on-arm/</guid>
      <description>&lt;p&gt;I really like Kubernetes for orchestrating Docker containers. If you&amp;rsquo;re don&amp;rsquo;t
familiar with Kubernetes I can highly recommend to take a look at
&lt;a href=&#34;http://kubernetes.io&#34;&gt;kubernetes.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can easily run you&amp;rsquo;re own Kubernetes cluster on your local machine using
Vagrant or run Kubernetes in the cloud using AWS, Azure or Google Compute.
But I like a more hands on solution and I always wanted my own &amp;ldquo;data center&amp;rdquo;.
On the other hand I don&amp;rsquo;t want to spend a fortune building a DC just for fun.&lt;/p&gt;

&lt;p&gt;A great alternative is to use ARM SoC boards like Raspberry PI. Thank&amp;rsquo;s to
&lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas Käldström&lt;/a&gt; and
&lt;a href=&#34;http://blog.hypriot.com/&#34;&gt;Hypriot&lt;/a&gt; this is a rather straight forward process.&lt;/p&gt;

&lt;p&gt;I used the &lt;code&gt;hypriotos&lt;/code&gt; image and Lucas &lt;code&gt;deb&lt;/code&gt;-package to install &lt;code&gt;kube-config&lt;/code&gt;.&lt;/p&gt;


&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Six Raspberry Pi:s in a cluster&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/IMG_1936.png&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;



&lt;p&gt;
&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Nodes in my cluster&lt;/span&gt;
          &lt;p&gt;
          Here you can see the nodes in the cluster. I&amp;#39;m using the service-loadbalancer addon (a topic for another post).
          
              
          
        &lt;/p&gt; 
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/k8s.png&#34; alt=&#34;Here you can see the nodes in the cluster. I&amp;#39;m using the service-loadbalancer addon (a topic for another post).&#34; /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Thank you Java, but it&#39;s time to Go</title>
      <link>http://larmog.github.io/2016/02/04/thank-you-java-but-its-time-to-go/</link>
      <pubDate>Thu, 04 Feb 2016 20:56:35 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/04/thank-you-java-but-its-time-to-go/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been a dedicated Java developer since 1998 and it&amp;rsquo;s been a great tool.
But recently there&amp;rsquo;s a couple of things I&amp;rsquo;ve been fretting about. I always been
slow using keyboard (never learnt how to use it correctly). When programming
that&amp;rsquo;s not always a bad thing, it gives you time to think about what you&amp;rsquo;re
writing.
But Java is very verbose and all those &lt;code&gt;private&lt;/code&gt;, &lt;code&gt;public&lt;/code&gt;, &lt;code&gt;;&lt;/code&gt;, &lt;code&gt;getXXX&lt;/code&gt;,
&lt;code&gt;setXXX&lt;/code&gt; kind of gets in the way.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;package main

import &amp;quot;fmt&amp;quot;

type Vertex struct {
	X int
	Y int
}

func main() {
	v := Vertex{1, 2}
	v.X = 4
	fmt.Println(v.X)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another thing that turns out to be complicated is coding standards. It&amp;rsquo;s all
well when your on a greenfield project and the team can decide their own rules.
But have you ever worked with legacy code that&amp;rsquo;s been abused for several years
buy developers who rather wish they were on that super cool project? I know you
can&amp;rsquo;t blame &lt;code&gt;Java&lt;/code&gt; for this but it&amp;rsquo;s still a problem. Coding standards are like
fashion and every developer has an opinion on where to put the &lt;code&gt;{}&lt;/code&gt; and don&amp;rsquo;t
mention line endings when your in a mixed Windows and *nix environment.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Gofmt is a tool that automatically formats Go source code
$go fmt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When Java was released in 1995 the promise &lt;em&gt;&amp;ldquo;write once, run anywhere&amp;rdquo;&lt;/em&gt; sounded
very tempting but has shown not to be altogether true. There have been, and
will always be, bugs and small differences in the JVM which means that your
program will always be dependent on the version of the JVM running your byte
code. That means you have at least two artifacts to consider and
one of them is out of your control.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Static compiled go binary that can be used in a Docker image.
# The binary is compiled using golang Docker image.
$CGO_ENABLED=0 GOOS=linux go build -ldflags &amp;quot;-s&amp;quot; -a -installsuffix cgo -o main
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If your like me feeling young and haven&amp;rsquo;t aged a day since 1995, the truth is
time goes by. Even if you do your best to keep young, it&amp;rsquo;s hard to change. Java
has done a good job: HotSpot, Regular expressions, NIO, Generics, Annotations,
primitive wrapper classes with autoboxing, Enumerations, Varargs and Lambda
Expressions. Just take a look at the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Java_version_history&#34;&gt;Java version history&lt;/a&gt;. But
all this adds up and gains weight. Java has become hard to learn and understand
and there are many pitfalls and compromises. Some times the right decision is
to start all over and make something new based on what you&amp;rsquo;ve learnt.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# OSX
$open http://tour.golang.org/welcome/1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;m keeping Java in my toolbox but I&amp;rsquo;ve started using
&lt;a href=&#34;https://golang.org/&#34;&gt;The Go Programming Language&lt;/a&gt; as my new tool and
it&amp;rsquo;s great fun.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simple Groovy AOP using MOP</title>
      <link>http://larmog.github.io/2014/08/18/simple-groovy-aop-using-mop/</link>
      <pubDate>Mon, 18 Aug 2014 21:37:45 +0100</pubDate>
      
      <guid>http://larmog.github.io/2014/08/18/simple-groovy-aop-using-mop/</guid>
      <description>&lt;p&gt;I was looking for a simple solution for adding cross cutting concerns to Groovy
classes. The most obvious solution was to implement &lt;code&gt;GroovyInterceptable&lt;/code&gt; but I wanted a
less intrusive solution. After a bit of googling I&amp;rsquo;ve stumbled across
&lt;code&gt;DelegatingMetaClass&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Here is a simple interceptor that works like a around advice. It only logs all method
calls for a class.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;
class Interceptor extends DelegatingMetaClass {
  	Interceptor(final Class cls) {
      	super(cls)
      	initialize()
  	}

  	public Object invokeMethod(Object obj, String method, Object[] args) {
      	String cls = obj.class.simpleName
      	println &amp;quot;before: $cls.$method, args:$args --&amp;gt;&amp;quot;
      	def val = null
      	try {
          	val = super.invokeMethod(obj, method, args)
      	} catch(Exception e) {
          	println &amp;quot;after: $cls.$method, has thrown:$e &amp;lt;--&amp;quot;
          	throw e
      	}
      	println &amp;quot;after: $cls.$method, return value:$val &amp;lt;--&amp;quot;
      	return val;
  	}

  	def static injectIn(Class cls) {
      	cls.metaClass = new Interceptor(cls)
  	}
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now use the &lt;code&gt;Interceptor&lt;/code&gt; to trace calls to &lt;code&gt;ArrayList&lt;/code&gt; like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;	Interceptor.InjectIn(ArrayList)

	def list = []
	list &amp;lt;&amp;lt; &amp;quot;Joe&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we run the above it will produce:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;	before: ArrayList.leftShift, args:[Joe] --&amp;gt;
	after: ArrayList.leftShift, return value:[Joe] &amp;lt;--
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It works width mixin&amp;rsquo;s too, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;	class Person {
		def name

		def sayHello() { println &amp;quot;Hi, my name is $name!&amp;quot;}
	}

	class Dancer {
		def dance() { println &amp;quot;I can dance&amp;quot; }
	}

	class Singer {
		def sing() { println &amp;quot;I can sing&amp;quot; }
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now create a person who dance and sing. The &lt;code&gt;Interceptor&lt;/code&gt; must be injected
after the mixin:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;	Person.mixin Singer
	Person.mixin Dancer
	Interceptor.injectIn(Person)

	def p = new Person(name: &amp;quot;Jill&amp;quot;)
	p.sayHello()
	p.sing()
	p.dance()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will produce the following output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;	before: Person.sayHello, args:[] --&amp;gt;
	before: Person.println, args:[Hi, my name is Jill!] --&amp;gt;
	Hi, my name is Jill!
	after: Person.println, return value:null &amp;lt;--
	after: Person.sayHello, return value:null &amp;lt;--
	before: Person.sing, args:[] --&amp;gt;
	I can sing
	after: Person.sing, return value:null &amp;lt;--
	before: Person.dance, args:[] --&amp;gt;
	I can dance
	after: Person.dance, return value:null &amp;lt;--
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we can see in the example above the &lt;code&gt;println&lt;/code&gt;-method is part of the Groovy object.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>