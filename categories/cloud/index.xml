<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cloud on kodbasen</title>
    <link>http://larmog.github.io/categories/cloud/</link>
    <description>Recent content in Cloud on kodbasen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Â© 2016 by larmog</copyright>
    <lastBuildDate>Sat, 15 Oct 2016 20:58:26 +0200</lastBuildDate>
    <atom:link href="http://larmog.github.io/categories/cloud/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Kubernetes on Scaleway - Part 3</title>
      <link>http://larmog.github.io/2016/10/15/kubernetes-on-scaleway---part-3/</link>
      <pubDate>Sat, 15 Oct 2016 20:58:26 +0200</pubDate>
      
      <guid>http://larmog.github.io/2016/10/15/kubernetes-on-scaleway---part-3/</guid>
      <description>

&lt;p&gt;This is the final part in a series about setting up Kubernetes on &lt;em&gt;Scaleway&lt;/em&gt;.
This final part is about setting up storage for your cluster.
&lt;img src=&#34;http://larmog.github.io/media/k8s-glusterfs-scaleway.png&#34; style=&#34;float:right;height:150px&#34;&gt;
Most of the steps here is already described in an earlier post:
&lt;a href=&#34;http://larmog.github.io/2016/02/22/glusterfs-on-kubernetes-arm/&#34;&gt;&lt;em&gt;GlusterFS On Kubernetes ARM&lt;/em&gt;&lt;/a&gt;
 that I wrote a couple of month back. You can also find the earlier posts in this
series here:
&lt;a href=&#34;http://larmog.github.io/2016/08/17/setting-up-kubernetes-on-scaleway---part-1/&#34;&gt;&lt;em&gt;Part 1&lt;/em&gt;&lt;/a&gt;
and &lt;a href=&#34;http://larmog.github.io/2016/10/09/kubernetes-on-scaleway---part-1-revisited--2/&#34;&gt;&lt;em&gt;Part 1 (revisited) and Part 2&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&#34;margin-bottom:40px&#34;&gt;&lt;/p&gt;

&lt;p&gt;The solution described here is not the only way you can use to set up &lt;em&gt;GlusterFS&lt;/em&gt;
for your cluster. You can also use a &lt;code&gt;DaemonSet&lt;/code&gt; or a &lt;code&gt;PetSet&lt;/code&gt; and run your
&lt;code&gt;glusterfs-servers&lt;/code&gt; as containers, but I like the separation of concerns, using
two dedicated servers.&lt;/p&gt;

&lt;h4 id=&#34;setting-up-glusterfs-server-nodes&#34;&gt;Setting up &lt;code&gt;glusterfs-server&lt;/code&gt; nodes&lt;/h4&gt;

&lt;p&gt;I&amp;rsquo;ve chosen to use two servers for a replica set of 2. The first thing we need
to do is to create two additional servers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ for i in {1..2}; do
    scw start $(scw create --name gfs-$i --commercial-type=&amp;quot;VC1S&amp;quot; Ubuntu_Xenial)
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and install &lt;code&gt;glusterfs-server&lt;/code&gt; and connect the servers using their private
DNS name:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ apt-get update &amp;amp;&amp;amp; apt-get install -y glusterfs-server attr
$ gluster peer probe xxxxxx.priv.cloud.scaleway.com
&lt;/code&gt;&lt;/pre&gt;

&lt;h5 id=&#34;creating-volumes&#34;&gt;Creating volumes&lt;/h5&gt;

&lt;p&gt;I create the volumes under the root partition, which is not recommended.
I leave it up to you mount an additional volume (disk) on your nodes for
&lt;code&gt;glusterfs volume&lt;/code&gt; storage. We will create four volumes.&lt;/p&gt;

&lt;p&gt;Create the volume directories on node &lt;code&gt;gfs-1&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ for i in {0..3}; do
    mkdir -p /data/brick2/vol$i
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and on &lt;code&gt;gfs-2&lt;/code&gt;, then create the volumes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Create and start the volumes
$ for i in {0..3}; do
    mkdir -p /data/brick2/vol$i \
    &amp;amp;&amp;amp; gluster volume create vol$i replica 2 \
    &amp;lt;your gfs-1 machine id&amp;gt;.priv.cloud.scaleway.com:/data/brick1/vol$i \
    &amp;lt;your gfs-2 machine id&amp;gt;.priv.cloud.scaleway.com:/data/brick2/vol$i force \
    &amp;amp;&amp;amp; gluster volume start vol$i
done
$ # Check your volumes
$ gluster volume info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;force&lt;/code&gt; flag is used because we&amp;rsquo;re using the root partition, and should not
be required if you use another partition.&lt;/p&gt;

&lt;h4 id=&#34;install-glusterfs-client-on-your-k8s-nodes&#34;&gt;Install &lt;code&gt;glusterfs-client&lt;/code&gt; on your k8s nodes&lt;/h4&gt;

&lt;p&gt;Next step is to install &lt;code&gt;glusterfs-client&lt;/code&gt; on all your &lt;em&gt;Kubernetes&lt;/em&gt; nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ apt-get install -y glusterfs-client attr
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;create-the-glusterfs-endpoint&#34;&gt;Create the &lt;code&gt;glusterfs-endpoint&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The final step to setup &lt;em&gt;GlusterFS&lt;/em&gt; is to create the endpoint.&lt;/p&gt;

&lt;p&gt;Use &lt;code&gt;kubectl&lt;/code&gt; and create the following &lt;code&gt;endpoint&lt;/code&gt; and &lt;code&gt;service&lt;/code&gt;, don&amp;rsquo;t
forget to replace the IP:s with your own privare IP:s of your &lt;code&gt;gfs-1&lt;/code&gt; and
&lt;code&gt;gfs-2&lt;/code&gt; servers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Endpoints
metadata:
  name: glusterfs-cluster
  namespace: default
subsets:
- addresses:
  - ip: 10.x.x.x
  - ip: 10.x.x.y
  ports:
  - port: 1
    protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: glusterfs-cluster
  namespace: default
spec:
  ports:
  - port: 1
    protocol: TCP
    targetPort: 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally we can create four &lt;code&gt;PersistentVolume&lt;/code&gt;:s using our &lt;em&gt;GlusterFS&lt;/em&gt;
volumes.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol0
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  glusterfs:
    endpoints: glusterfs-cluster
    path: vol0
    readOnly: false
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  glusterfs:
    endpoints: glusterfs-cluster
    path: vol1
    readOnly: false
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol2
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  glusterfs:
    endpoints: glusterfs-cluster
    path: vol2
    readOnly: false
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol3
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  glusterfs:
    endpoints: glusterfs-cluster
    path: vol3
    readOnly: false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also create &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;:s that uses your &lt;code&gt;PersistentVolume&lt;/code&gt;:s
but maybe it&amp;rsquo;s better to wait until it&amp;rsquo;s time to use them from your apps.
Notice that I have used &lt;code&gt;ReadWriteOnce&lt;/code&gt; on all nodes and
&lt;code&gt;persistentVolumeReclaimPolicy: Retain&lt;/code&gt;. This means that the volumes can not be
shared and you need to reclaim them manually.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-0
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-2
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-3
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;a-final-word-about-file-permissions&#34;&gt;A final word about file permissions&lt;/h4&gt;

&lt;p&gt;Depending on if your containers is running as root (&lt;code&gt;UID 0&lt;/code&gt;) or any other &lt;code&gt;UID&lt;/code&gt;
you may need to configure your &lt;code&gt;securityContext&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&#34;wrap-up&#34;&gt;Wrap up&lt;/h4&gt;

&lt;p&gt;This was the final part on setting up &lt;em&gt;Kubernetes&lt;/em&gt; on &lt;em&gt;Scaleway&lt;/em&gt;. The
installation can obviously be improved, for example the &lt;code&gt;traefik&lt;/code&gt; proxy
does currently not support &lt;code&gt;url re-writes&lt;/code&gt; which may be a problem for you. But
you can always replace it with &lt;code&gt;nginx&lt;/code&gt;. To protect your &lt;code&gt;glusterfs-server&lt;/code&gt; nodes
and save some IP-adresses, you can remove the public IP:s from your servers.
But remember you need to assign one, if you want to upgrade the server.&lt;/p&gt;

&lt;p&gt;As always, feedback is very welcome.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes on Scaleway - Part 1 (Revisited) &amp; 2</title>
      <link>http://larmog.github.io/2016/10/09/kubernetes-on-scaleway---part-1-revisited--2/</link>
      <pubDate>Sun, 09 Oct 2016 22:00:00 +0200</pubDate>
      
      <guid>http://larmog.github.io/2016/10/09/kubernetes-on-scaleway---part-1-revisited--2/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://scaleway.com&#34;&gt;
&lt;img src=&#34;https://www.scaleway.com/img/scaleway.f0e4.svg&#34; style=&#34;float:right;height:80px&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Things doesn&amp;rsquo;t always turn out as planned. This is specially true when it comes
to this series, on how to setup &lt;em&gt;Kubernetes&lt;/em&gt; on &lt;em&gt;Scaleway&lt;/em&gt;. The plan was that
part 2 would be about setting up an &lt;code&gt;ingress-controller&lt;/code&gt; and securing the
&lt;code&gt;api-server&lt;/code&gt; and &lt;code&gt;dashboard&lt;/code&gt;. But in the meantime, while I was struggling with
getting &lt;code&gt;weave&lt;/code&gt; working on &lt;code&gt;ARM&lt;/code&gt; and re-writing my own little project &lt;code&gt;sloop&lt;/code&gt;,
Kubernetes &lt;code&gt;v1.4.0&lt;/code&gt; was released with the &lt;code&gt;kubeadm&lt;/code&gt; tool.&lt;/p&gt;

&lt;p&gt;With the tool &lt;code&gt;kubeadm&lt;/code&gt;, you can easily install a secure Kubernetes cluster. It
makes projects like my own &lt;code&gt;Sloop&lt;/code&gt; unnecessary, and that&amp;rsquo;s a good thing.&lt;/p&gt;

&lt;p&gt;In this post I will show how to setup Kubernetes using &lt;code&gt;kubeadm&lt;/code&gt; and setting up
an &lt;code&gt;ingress-controller&lt;/code&gt; and how to secure your &lt;code&gt;api-server&lt;/code&gt; and &lt;code&gt;dashboard&lt;/code&gt;. We
won&amp;rsquo;t be using &lt;code&gt;ARM&lt;/code&gt; for this, that&amp;rsquo;s a topic for another blog post.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the recipe:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1 Scaleway account
2 VC1S servers
1 wildcard certificate
2 kubeadm
1 DaemonSet weave-kube CNI plugin
1 traefik reverse proxy (http://traefik.io)
1 traefik ingress-controller
1 domain name
1 DNS A record
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can replace the domain and &lt;code&gt;DNS A&lt;/code&gt; record with entries in your &lt;code&gt;hosts&lt;/code&gt; file
if you just want to try this out.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s get started!&lt;/p&gt;

&lt;h4 id=&#34;create-master-node&#34;&gt;Create master node&lt;/h4&gt;

&lt;p&gt;Creating your Kubernetes cluster using &lt;code&gt;kubeadm&lt;/code&gt; is described excellent in this
&lt;a href=&#34;http://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;guide&lt;/a&gt;. I only
include the steps here for completeness.&lt;/p&gt;

&lt;p&gt;First thing is to create the &lt;code&gt;k8s-master&lt;/code&gt; node:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Create your master node:
$ scw start -w $(scw create --name=&amp;quot;k8s-master&amp;quot; --commercial-type=&amp;quot;VC1S&amp;quot;  Docker)
$ scw ps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Login to your &lt;code&gt;k8s-master&lt;/code&gt; node and install the Kubernetes components:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
$ cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
$ apt-get update
$ apt-get install -y kubelet kubeadm kubectl kubernetes-cni
$ kubeadm init
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;create-your-worker-node&#34;&gt;Create your worker node&lt;/h4&gt;

&lt;p&gt;Repeat the steps from installing the &lt;code&gt;k8s-master&lt;/code&gt; but name the node: &lt;code&gt;k8s-node1&lt;/code&gt;
and use the &lt;code&gt;join&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubeadm join --token &amp;lt;token&amp;gt; &amp;lt;k8s-master IP&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;install-a-pod-network&#34;&gt;Install a pod network&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl apply -f https://git.io/weave-kube
daemonset &amp;quot;weave-net&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you run &lt;code&gt;kubectl get nodes&lt;/code&gt;, on your master, you should now see two nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get nodes
NAME         STATUS    AGE
k8s-master   Ready     1d
k8s-node1    Ready     1d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install the latest &lt;code&gt;dashboard&lt;/code&gt; and find the service ip:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml
$ kubectl get svc -n kube-system | grep dashboard
kubernetes-dashboard   100.xx.xx.xxx    &amp;lt;nodes&amp;gt;       80/TCP          2h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will be using the &lt;code&gt;dashboard&lt;/code&gt; service ip later on. That was easy, don&amp;rsquo;t you
think? You now have your own Kubernetes cluster running.&lt;/p&gt;

&lt;h4 id=&#34;using-traefik-as-ingress-controller-and-reverse-proxy&#34;&gt;Using &lt;code&gt;traefik&lt;/code&gt; as &lt;code&gt;ingress-controller&lt;/code&gt; and reverse proxy&lt;/h4&gt;

&lt;p&gt;Using &lt;code&gt;CNI&lt;/code&gt; for the pod network comes with a price tag. You can&amp;rsquo;t use &lt;code&gt;hostPort&lt;/code&gt;
in your pods right know. Im sure this will be fixed in later versions but right
now that&amp;rsquo;s a problem. A &lt;code&gt;ingress-controller&lt;/code&gt; uses &lt;code&gt;hostPort&lt;/code&gt; to expose the
cluster.&lt;/p&gt;

&lt;p&gt;The solution I found is to use a reverse proxy running as a static pod on one
of the nodes (I choose &lt;code&gt;k8s-node1&lt;/code&gt;) and configuring it with &lt;code&gt;hostNetwork: true&lt;/code&gt;.
Traefik is a reverse proxy written in Go and we will be using it for securing
the &lt;code&gt;dashboard&lt;/code&gt; with basic authentication and for proxying request to the
&lt;code&gt;ingress-controller&lt;/code&gt; which happens also be &lt;code&gt;traefik&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with installing &lt;code&gt;traefik&lt;/code&gt; as &lt;code&gt;ingress-controller&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    k8s-app: traefik-ingress-lb
  name: traefik-lb
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: traefik-ingress-lb
  template:
    metadata:
      creationTimestamp: null
      labels:
        k8s-app: traefik-ingress-lb
    spec:
      containers:
      - args:
        - --kubernetes
        image: traefik:v1.0.0
        imagePullPolicy: IfNotPresent
        name: traefik-lb
        ports:
        - containerPort: 80
          name: web
          protocol: TCP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see there&amp;rsquo;s no &lt;code&gt;hostPort&lt;/code&gt;. Next step is to expose the &lt;code&gt;traefik-lb&lt;/code&gt;
as a service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deployment traefik-lb --port=80 --target-port=80 -n kube-system
$ kubectl get svc -n kube-system | grep traefik-lb
traefik-lb          100.xx.xxx.xxx   &amp;lt;none&amp;gt;        80/TCP          1d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now have two services &lt;code&gt;kubernetes-dashboard&lt;/code&gt; and &lt;code&gt;traefik-lb&lt;/code&gt;. These are
the &lt;code&gt;backends&lt;/code&gt; for our &lt;code&gt;traefik&lt;/code&gt; reverse proxy.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the &lt;code&gt;traefic.toml&lt;/code&gt; file for our proxy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;logLevel = &amp;quot;DEBUG&amp;quot;
defaultEntryPoints = [&amp;quot;http&amp;quot;, &amp;quot;https&amp;quot;]

[entryPoints]
  [entryPoints.http]
  address = &amp;quot;:80&amp;quot;
    [entryPoints.http.redirect]
    entryPoint = &amp;quot;https&amp;quot;
  [entryPoints.https]
  address = &amp;quot;:443&amp;quot;
    [entryPoints.https.tls]
      [[entryPoints.https.tls.certificates]]
      CertFile = &amp;quot;/etc/traefik/server.crt&amp;quot;
      KeyFile = &amp;quot;/etc/traefik/server.key&amp;quot;
  [entryPoints.https2]
  address = &amp;quot;:8443&amp;quot;
    [entryPoints.https2.tls]
      [[entryPoints.https2.tls.certificates]]
      CertFile = &amp;quot;/etc/traefik/server.crt&amp;quot;
      KeyFile = &amp;quot;/etc/traefik/server.key&amp;quot;
    [entryPoints.https2.auth.basic]
    users = [ &amp;quot;test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/&amp;quot; ]

[web]
address = &amp;quot;:6443&amp;quot;
CertFile = &amp;quot;/etc/traefik/server.crt&amp;quot;
KeyFile = &amp;quot;/etc/traefik/server.key&amp;quot;
ReadOnly = true
  [web.auth.basic]
  users = [ &amp;quot;test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/&amp;quot; ]

[file]
filename = &amp;quot;/etc/traefik/rules.toml&amp;quot;
watch = true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the file to &lt;code&gt;/etc/traefik&lt;/code&gt; directory on the &lt;code&gt;k8s-node1&lt;/code&gt;.
Use &lt;code&gt;htpasswd&lt;/code&gt; to generate a user name and password and edit the file. Change
the &lt;code&gt;CertFile:&lt;/code&gt; and &lt;code&gt;KeyFile:&lt;/code&gt; to point to your wildcard certificate and key.
We&amp;rsquo;re using three &lt;code&gt;entryPoints&lt;/code&gt;. Port &lt;code&gt;80&lt;/code&gt; and &lt;code&gt;443&lt;/code&gt; is reserved for our
services in the cluster, &lt;code&gt;8443&lt;/code&gt; is used for the &lt;code&gt;kubernetes-dashboard&lt;/code&gt; and port
&lt;code&gt;6443&lt;/code&gt; is used for the &lt;code&gt;traefik webui&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next we need to configure traefik &lt;code&gt;frontend&lt;/code&gt;:s and &lt;code&gt;backend&lt;/code&gt;:s in a configuration
file: &lt;code&gt;/etc/traefik/rules.toml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[frontends]

  [frontends.frontend1]
  passHostHeader = true
  backend = &amp;quot;dashboard-backend&amp;quot;
  entrypoints = [&amp;quot;https2&amp;quot;]
    [frontends.frontend1.routes.test_1]
    rule = &amp;quot;Host:dashboard.mydomain.com&amp;quot;

  [frontends.frontend2]
  passHostHeader = true
  backend = &amp;quot;k8s&amp;quot;
    [frontends.frontend2.routes.default]
    rule = &amp;quot;Host:{subdomain:[a-z]+}.mydomain.com&amp;quot;

[backends]
  [backends.dashboard-backend]

    [backends.dashboard-backend.LoadBalancer]
    method = &amp;quot;wrr&amp;quot;

    [backends.dashboard-backend.servers.server1]
    url = &amp;quot;http://100.xx.xx.xxx:80&amp;quot;

   [backends.k8s]

     [backends.k8s.LoadBalancer]
     method = &amp;quot;wrr&amp;quot;

     [backends.k8s.servers.server1]
     url = &amp;quot;http://100.xx.xx.xxx:80&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Change the ip of the &lt;code&gt;backends.dashboard-backend.servers.server1&lt;/code&gt; &lt;code&gt;url&lt;/code&gt; to your
&lt;code&gt;kubernetes-dashboard&lt;/code&gt; service and change the ip of the
&lt;code&gt;backends.k8s.servers.server1&lt;/code&gt; &lt;code&gt;url&lt;/code&gt; to the &lt;code&gt;traefik-lb&lt;/code&gt; service. You also need
to edit the domain so that it matches your wildcard certificate.&lt;/p&gt;

&lt;p&gt;Now it&amp;rsquo;s time to create our static pod. Copy the following to
&lt;code&gt;/etc/kubernetes/manifests/traefik.yaml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  labels:
    run: traefik
  name: traefik
  namespace: kube-system
spec:
  containers:
  - image: traefik:v1.1.0-rc1
    imagePullPolicy: IfNotPresent
    name: traefik
    resources: {}
    volumeMounts:
    - mountPath: /etc/traefik
      name: config
      readOnly: false
    ports:
    - name: http
      containerPort: 80
      hostPort: 80
      protocol: TCP
    - name: https
      containerPort: 443
      hostPort: 443
      protocol: TCP
    - name: webui
      containerPort: 6443
      hostPort: 6443
      protocol: TCP
    - name: dashboard
      containerPort: 8443
      hostPort: 8443
      protocol: TCP
  hostNetwork: true
  restartPolicy: Always
  securityContext: {}
  volumes:
  - name: config
    hostPath:
      path: /etc/traefik
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final step is to create a DNS A record pointing to the &lt;code&gt;k8s-node1&lt;/code&gt;:s public&lt;br /&gt;
IP.&lt;/p&gt;

&lt;p&gt;Time to check if everything is working:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ # The dashbord requires authentication
$ curl -s https://dashbord.mydomain.com:8443
401 Unauthorized
$
$ # The traefik webui also requires authentication
$ curl -s https://webui.mydomain.com:6443
401 Unauthorized
$
$ # All other requests on port 80 and 443 is forward to k8s and traefik-lb
$ curl -s http://anyapp.mydomain.com.org
404 page not found
$ curl -s https://anyapp.mydomain.com.org
404 page not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ok, it seems to work. I leave it up to you to explore the &lt;code&gt;dashboard&lt;/code&gt; and
&lt;code&gt;traefik&lt;/code&gt; &lt;code&gt;webui&lt;/code&gt; in you favorite browser.&lt;/p&gt;

&lt;h4 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;We now got a secured Kubernetes cluster that we can use for hosting all our apps
and services. In the last post in this series we will setup &lt;code&gt;PersistentVolume&lt;/code&gt;:s
using &lt;em&gt;GlusterFS&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>