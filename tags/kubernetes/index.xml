<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on kodbasen</title>
    <link>http://larmog.github.io/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on kodbasen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright © 2016 by larmog</copyright>
    <lastBuildDate>Mon, 22 Feb 2016 09:35:30 +0100</lastBuildDate>
    <atom:link href="http://larmog.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>GlusterFS On Kubernetes ARM</title>
      <link>http://larmog.github.io/2016/02/22/glusterfs-on-kubernetes-arm/</link>
      <pubDate>Mon, 22 Feb 2016 09:35:30 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/22/glusterfs-on-kubernetes-arm/</guid>
      <description>&lt;p&gt;If you followed my earlier posts, you know that I’m running a Kubernetes cluster
on Raspberry Pi, using &lt;strong&gt;HypriotOS&lt;/strong&gt; and Lucas Käldströms &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes-On-ARM&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;If not you can find my earlier posts here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;Kubernetes-On-ARM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/&#34;&gt;Gogs and Drone On Kubernetes-ARM - Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/&#34;&gt;Gogs and Drone On Kubernetes-ARM - Part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-3/&#34;&gt;Gogs and Drone On Kubernetes-ARM - Part 3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&amp;rsquo;ve used &lt;strong&gt;NFS&lt;/strong&gt; volumes for storage and DiskStation NAS as NFS server.
The solution have had some drawbacks and it&amp;rsquo;s been hard to synchronize users and
groups, and I&amp;rsquo;ve used &lt;code&gt;all_squash&lt;/code&gt; to a specific &lt;code&gt;uid&lt;/code&gt;. That worked for &lt;strong&gt;Gogs&lt;/strong&gt;
and &lt;strong&gt;Drone&lt;/strong&gt; but not for &lt;strong&gt;MySQL&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Another alternative is to use &lt;strong&gt;GlusterFS&lt;/strong&gt; volumes. GlusterFS is a clustered
file-system that is capable of scaling to several peta-bytes.
GlusterFS aggregates storage bricks, that can be made of commodity hardware.
When I started this project, I had two
&lt;code&gt;Raspberry Pi 1 B+&lt;/code&gt; in my desk drawer. I&amp;rsquo;ve used them as nodes in the cluster,
but the small amount of &lt;code&gt;RAM&lt;/code&gt; has shown them less useful compared to the newer
&lt;code&gt;Raspberry Pi 2 B&lt;/code&gt; based nodes. But then it hit me that maybe I could use the
old Pi 1 boards as bricks in GlusterFS. One of the purposes with this project is
to see how much of a Kubernetes cluster you can get for a reasonable amount. So
I&amp;rsquo;d like to keep a tight budget. Normally when setting up a brick you want to
use RAID, because you don&amp;rsquo;t want to handle disk failures on a brick level, but
for this project it&amp;rsquo;s a reasonable solution.&lt;/p&gt;

&lt;p&gt;After a trip to my local Electronic Retailers I came home with this
(not the cluster):

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;GlusterFS ingredients&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/IMG_1953.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;* 2 WD external usb harddrives&lt;/li&gt;
&lt;li&gt;* 2 Raspberry Pi 2 B boards (to replace the Pi 1:s)&lt;/li&gt;
&lt;li&gt;* 2 SD cards&lt;/li&gt;
&lt;li&gt;* One more Multi-Pi Stackable Raspberry Pi Case&lt;/li&gt;
&lt;li&gt;* Cables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My first attempt was to connect the external drives to the usb port. That didn&amp;rsquo;t
work of course, external disks needs more power than what the Pi usb port can
deliver. There are two alternatives for solving this. You could use an external
usb-hub with it&amp;rsquo;s own power supply. But that means two usb-hubs and two power
adapters. The other alternative is to use a usb Y-cable and connect the extra
connector to the usb supercharger (that powers all nodes). After one more trip
to the store to get a Y-cable and then connect it to the usb supercharger, the
problem was solved.&lt;/p&gt;

&lt;p&gt;The image shows the cluster after assembling.

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Cluster ready for GlusterFS&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/IMG_1961.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s room for two more Pi:s in the switch and usb supercharger.&lt;/p&gt;

&lt;p&gt;Next step was to partition and format the disks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Prepare your disk and create a partition first
$ # Install xfs tools
$ apt-get install -y xfsprogs
$ # Format the disk
$ mkfs.xfs -f -L brick1 -i size=512 /dev/sda1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and mount and install &lt;code&gt;glusterfs&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Create and mount the brick
$ mkdir -p /data/brick1
$ echo &#39;/dev/sda1 /data/brick1 xfs defaults 1 2&#39; &amp;gt;&amp;gt; /etc/fstab
$ cat /etc/mtab
# Install GlusterFS server
$ apt-get install -y glusterfs-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The steps above was made on both servers.&lt;/p&gt;

&lt;p&gt;Next we needed to connect our storage servers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ #From server1
$ gluster peer probe server2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ #From server2
$ gluster peer probe server1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Time to create the first volume:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ mkdir /data/brick1/vol0
$ gluster volume create vol0 replica 2 store1.local:/data/brick1/vol0 store2.local:/data/brick2/vol0
$ gluster volume start vol0
$ gluster volume info
$ gluster volume info

Volume Name: vol0
Type: Replicate
Volume ID: 6ffe7723-e0ac-44c7-a004-fa951467884d
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: server1:/data/brick1/vol0
Brick2: server2:/data/brick2/vol0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we&amp;rsquo;re done.&lt;/p&gt;

&lt;p&gt;The only thing left is to set up our &lt;code&gt;glusterfs-cluster endpoints&lt;/code&gt; as described
in the &lt;a href=&#34;http://kubernetes.io/v1.1/examples/glusterfs/README.html&#34;&gt;Kubernetes documentation for GlusterFS Volumes&lt;/a&gt; and configure
our &lt;a href=&#34;http://kubernetes.io/v1.1/docs/user-guide/persistent-volumes.html&#34;&gt;Persistent Volumes and Claims&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Migrating the volumes from NFS to GlusterFS was very easy. The only problem
I&amp;rsquo;ve had was a silly mistake. Coming from NFS I first tried to share a volume
between applications, using different paths: &lt;code&gt;/vol 0/mysql&lt;/code&gt;, &lt;code&gt;/vol/drone&lt;/code&gt; that
didn&amp;rsquo;t work. You need to create different volumes for each application. Except
from the small misconception, there&amp;rsquo;s been no problems at all.&lt;/p&gt;

&lt;p&gt;Now finally I have a Kubernetes cluster running on bare metal that can be used
for trying out techniques, for building microservices.&lt;/p&gt;

&lt;p&gt;By the way, I took the opportunity to overclock my Raspberry Pi:s.
Thanks to &lt;a href=&#34;http://haydenjames.io/raspberry-pi-2-overclock/&#34;&gt;Hayden James&lt;/a&gt;, all
cluster nodes now runs at 1000 Mhz, and it seems stable.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dashboard on Kubernetes ARM</title>
      <link>http://larmog.github.io/2016/02/13/dashboard-on-kubernetes-arm/</link>
      <pubDate>Sat, 13 Feb 2016 11:50:12 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/13/dashboard-on-kubernetes-arm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas Käldström&lt;/a&gt; added the new &lt;strong&gt;Dashboard&lt;/strong&gt; addon
in the &lt;code&gt;dev&lt;/code&gt; bransch to &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes On ARM&lt;/a&gt;
and I decided to try it out.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how to install it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ mkdir /etc/kubernetes/source/addons/dashboard
$ curl -sSL https://raw.githubusercontent.com/luxas/kubernetes-on-arm/dev/addons/dashboard.yaml &amp;gt; \ /etc/kubernetes/source/addons/dashboarddashboard.yaml
$ kube-config enable-addon dashboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here it is:

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Kubernetes Dashboard on ARM&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/dashboard.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;Cool, Thanks &lt;a href=&#34;https://twitter.com/kubernetesonarm&#34;&gt;Lucas&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gogs and Drone On Kubernetes-ARM - Part 3</title>
      <link>http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-3/</link>
      <pubDate>Mon, 08 Feb 2016 16:39:34 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-3/</guid>
      <description>

&lt;p&gt;This is the third part in a series on setting up Gogs and Drone on
Kubernetes-ARM. You can find the earlier posts here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;Kubernetes-On-ARM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/&#34;&gt;Gogs and Drone &amp;hellip; Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/&#34;&gt;Gogs and Drone &amp;hellip; Part 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this third episode I will try to show how to setup Drone on Kubernetes-ARM.
A disclaimer, I&amp;rsquo;m new to Drone and out on deep water so&amp;hellip;&lt;/p&gt;

&lt;p&gt;If your wondering about Drone take a look at
&lt;a href=&#34;https://github.com/drone/drone&#34;&gt;GitHub page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;ldquo;Every build is executed inside an ephemeral Docker container, giving
developers complete control over their build environment with guaranteed
isolation.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This means that the &lt;code&gt;build agent&lt;/code&gt; is a container and all the plugins are also
containers. This makes it a bit problematic running Drone on ARM because we
need to rebuild everything, from Drone it self to all plugins that we wan&amp;rsquo;t to
use. We need to setup a Drone infrastructure for ARM, &lt;strong&gt;or so I thought&amp;hellip;&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&#34;on-the-shoulder-of-others:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;On the shoulder of others&lt;/h5&gt;

&lt;p&gt;Fortunately before I jumped in to the &lt;em&gt;rabbit hole&lt;/em&gt;, not knowing if it was a
dead end, I found two different initiatives for running Drone on ARM. You can
read more about it here:
&lt;a href=&#34;https://discuss.drone.io/t/drone-ported-to-arm/55&#34;&gt;Drone ported to ARM&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I decided to try
&lt;a href=&#34;https://github.com/armhf-docker-library/drone&#34;&gt;armhf-docker-library/drone&lt;/a&gt; and
I also found this example from Greg Taylor: &lt;a href=&#34;https://github.com/drone-demos/drone-on-kubernetes/&#34;&gt;drone-on-kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;h5 id=&#34;lets-get-started:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;Lets get started&lt;/h5&gt;

&lt;p&gt;First we need to create persistent volume for our sqlite database:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-drone
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: &amp;lt;Path exported NFS volume&amp;gt;
    server: &amp;lt;IP NFS Server&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and a volume claim:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-drone
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now create our replication controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ReplicationController
metadata:
  labels:
    app: droneio
  name: droneio
  namespace: default
spec:
  replicas: 1
  selector:
    app: droneio
  template:
    metadata:
      labels:
        app: droneio
    spec:
      containers:
      # Drone ARM port https://github.com/armhf-docker-library
      - image: armhfbuild/drone:latest
        imagePullPolicy: Always
        name: droneio
        ports:
          - containerPort: 8000
            name: web   
            protocol: TCP
        env:
        - name: REMOTE_DRIVER
          value: &amp;quot;gogs&amp;quot;
        - name: REMOTE_CONFIG
          value: &amp;quot;https://&amp;lt;YOUR GOGS SERVICE&amp;gt;?skip_verify=true&amp;amp;open=false&amp;quot;
        - name: DEBUG
          value: &amp;quot;true&amp;quot;
        #Drone ARM port plugins
        - name: PLUGIN_FILTER
          value: &amp;quot;armhfplugins/*&amp;quot;
        volumeMounts:
          - mountPath: &amp;quot;/var/lib/drone&amp;quot;
            name: persistentdata
          - mountPath: /var/run/docker.sock
            name: docker-socket
          - mountPath: /var/lib/docker
            name: docker-lib
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      volumes:
          - name: persistentdata
            persistentVolumeClaim:
              claimName: pvc-drone
          - name: docker-socket
            hostPath:
              path: /var/run/docker.sock
          - name: docker-lib
            hostPath:
              path: /var/lib/docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at our controllers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get rc
CONTROLLER   CONTAINER(S)   IMAGE(S)                        SELECTOR       REPLICAS   AGE
droneio      droneio        armhfbuild/drone:latest         app=droneio    1          2h
gogs         gogs           larmog/rpi-gogs:0.8.23.0126-2   app=gogs       1          10d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we create our service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  annotations:
    # For our service-loadbalancer
    serviceloadbalancer/lb.sslTerm: &amp;quot;true&amp;quot;
    serviceloadbalancer/lb.host: &amp;lt;virtual host&amp;gt;
    serviceloadbalancer/lb.cookie-sticky-session: &amp;quot;true&amp;quot;
  labels:
    app: droneio
  name: droneio
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8000
  selector:
    app: droneio
  sessionAffinity: None
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here´s our services:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get svc
NAME         CLUSTER_IP   EXTERNAL_IP   PORT(S)    SELECTOR       AGE
droneio      10.0.0.11    &amp;lt;none&amp;gt;        80/TCP     app=droneio    2h
gogs         10.0.0.85    &amp;lt;none&amp;gt;        80/TCP     app=gogs       13d
gogs-ssh     10.0.0.216   nodes         2222/TCP   app=gogs       13d
kubernetes   10.0.0.1     &amp;lt;none&amp;gt;        443/TCP    &amp;lt;none&amp;gt;         29d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If all has gone according to our plan we should see the service in our
&lt;code&gt;ha-proxy&lt;/code&gt; status page, and you should be able to login with your &lt;em&gt;Gogs&lt;/em&gt;
account.&lt;/p&gt;

&lt;h5 id=&#34;ca-and-pki-infrastructure:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;CA and PKI infrastructure&lt;/h5&gt;

&lt;p&gt;If your using your own CA and PKI infrastructure and a service-loadbalancer,
then there is a couple of things you need to do. When you have activated a
repository in Drone, you need to edit the Webhook that notifies Drone.
Use the service name or the IP address of your Drone service
(&lt;code&gt;droneio.&amp;lt;namespace&amp;gt;&lt;/code&gt;) in Kubernetes.

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Gogs Webhook&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/gogs-webhook.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;There is no easy way (at least none that i know of) to add your CA certificate.
This means that the &lt;code&gt;plugins/drone-git&lt;/code&gt; will fail to clone your repository using
&lt;code&gt;https&lt;/code&gt;. It gave me a serious headache until i found a solution and I admit it&amp;rsquo;s
a bit of a hack.

&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;drone-git failed to clone&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/drone-git-failed.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;

&lt;p&gt;You need to set &lt;code&gt;GIT_SSL_NO_VERIFY=true&lt;/code&gt; in your &lt;code&gt;.drone.yml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;clone:
    environment:
      - GIT_SSL_NO_VERIFY=true
    path: /xxxx
&lt;/code&gt;&lt;/pre&gt;


&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;drone-git cloned successfully&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/drone-git-success.jpg&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;



&lt;h5 id=&#34;conclusion:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;Conclusion&lt;/h5&gt;

&lt;p&gt;This concludes our series about running Gogs and Drone on Kubernetes-ARM.
Thanks to the excellent job done by others, it is a rather straight forward
process to set it up. I hope you liked the series and hope you&amp;rsquo;ve picked up
something you didn&amp;rsquo;t know before. Now I&amp;rsquo;m off to learn some more about &lt;em&gt;Drone&lt;/em&gt;.&lt;/p&gt;

&lt;h5 id=&#34;finally:45fd2cf7d6daaa0f28a943db69cd162c&#34;&gt;Finally&lt;/h5&gt;

&lt;p&gt;Some useful links:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.hypriot.com/&#34;&gt;http://blog.hypriot.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;https://github.com/luxas/kubernetes-on-arm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jinkit.com/k8s-on-rpi/&#34;&gt;http://www.jinkit.com/k8s-on-rpi/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://discuss.drone.io/t/drone-ported-to-arm/55&#34;&gt;https://discuss.drone.io/t/drone-ported-to-arm/55&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kelseyhightower/docker-kubernetes-tls-guide&#34;&gt;https://github.com/kelseyhightower/docker-kubernetes-tls-guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kelseyhightower/conf2kube&#34;&gt;https://github.com/kelseyhightower/conf2kube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/drone-demos&#34;&gt;https://github.com/drone-demos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Gogs and Drone On Kubernetes-ARM - Part 2</title>
      <link>http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/</link>
      <pubDate>Mon, 08 Feb 2016 09:21:47 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/</guid>
      <description>&lt;p&gt;This is the second part in a series of posts describing how I have setup Gogs
and Drone on
&lt;a href=&#34;http://larmog.github.io/2016/02/06/kubernetes-on-arm/&#34;&gt;Kubernetes-On-ARM&lt;/a&gt; cluster.
In &lt;a href=&#34;http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/&#34;&gt;Part 1&lt;/a&gt; we
talked about setting up Gogs.&lt;/p&gt;

&lt;p&gt;In this part I&amp;rsquo;ll explain how to setup &lt;code&gt;service-loadbalancer&lt;/code&gt; to expose services
outside the cluster.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m using &lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas Käldström&lt;/a&gt;:s great
&lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;kubernetes-on-arm&lt;/a&gt; project.
There&amp;rsquo;s a load-balancer addon based on &lt;a href=&#34;https://github.com/kubernetes/contrib&#34;&gt;kubernetes/contrib/service-loadbalancer&lt;/a&gt;
that wasn&amp;rsquo;t ready in the &lt;code&gt;0.6.3&lt;/code&gt; release.&lt;/p&gt;

&lt;p&gt;Before we begin you might take a look at
&lt;a href=&#34;https://github.com/kubernetes/contrib/tree/master/service-loadbalancer&#34;&gt;service-loadbalancer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What I did was to build the &lt;code&gt;service-loadbalancer&lt;/code&gt; from the
&lt;code&gt;kubernetes/contrib master branch&lt;/code&gt;. Once again Lucas have made a great job and
created a docker file for building the Kubernetes-ARM binaries on x86.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ # Clone the repository
$ git clone https://github.com/luxas/kubernetes-on-arm
$ # Build the Docker image
$ cd kubernetes-on-arm/scripts/build-k8s-on-amd64
$ docker build -t build-k8s-on-amd64 .
$ # Create a container
$ docker run --name=build-k8s-on-amd64 build-k8s-on-amd64 true
$ # Copy out the binaries from the container
$ docker cp build-k8s-on-amd64:/output .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;service-loadbalancer&lt;/code&gt; binary is located in the &lt;code&gt;output&lt;/code&gt; directory.
Transfer the file to the nodes that will have the role of load-balancer.
e.g.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ scp ./output/service-loadbalancer \ xxx.xxx.xxx.xxx:/etc/kubernetes/source/images/kubernetesonarm/_bin/latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;service-loadbalancer&lt;/code&gt; uses a template for creating a &lt;code&gt;ha-proxy.cfg&lt;/code&gt;.
The template I&amp;rsquo;m using can be found here: &lt;a href=&#34;https://goo.gl/TzvKhX&#34;&gt;template.cfg&lt;/a&gt;
On each node build the &lt;code&gt;kubernetesonarm/loadbalancer&lt;/code&gt; image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd /etc/kubernetes/source/images/kubernetesonarm/loadbalancer
$ mv template.cfg template.cfg.org
$ wget wget https://goo.gl/TzvKhX -O template.cfg
$ ./build.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Phew&amp;hellip; were half way. The Docker image is in place on our nodes. Now we need
to label them so that the scheduler can place the &lt;code&gt;service-loadbalancer&lt;/code&gt; on the
right nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl label --overwrite nodes &amp;lt;node name&amp;gt; role=loadbalancer
$ kubectl get nodes
NAME           LABELS                                                  STATUS    AGE
&amp;lt;node name&amp;gt;   kubernetes.io/hostname=&amp;lt;node name&amp;gt;,role=loadbalancer     Ready     2d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it&amp;rsquo;s time to create the &lt;code&gt;loadbalancer&lt;/code&gt;. If you wan&amp;rsquo;t to use &lt;code&gt;https&lt;/code&gt; you need
to create a &lt;code&gt;secret&lt;/code&gt; and mount the volume in your pod template.
Here&amp;rsquo;s my &lt;code&gt;loadbalancer-rc.yaml&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ReplicationController
metadata:
  name: service-loadbalancer
  namespace: kube-system
  labels:
    app: service-loadbalancer
    version: v1
spec:
  replicas: 1
  selector:
    app: service-loadbalancer
    version: v1
  template:
    metadata:
      labels:
        app: service-loadbalancer
        version: v1
    spec:
      nodeSelector:
        role: loadbalancer
      volumes:
      - name: ssl-volume
        secret:
          secretName: kodbasen-ssl-secret
      containers:
      - image: kubernetesonarm/loadbalancer
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        name: haproxy
        ports:
        # All http services
        - containerPort: 80
          hostPort: 80
          protocol: TCP
        # nginx https
        - containerPort: 443
          hostPort: 443
          protocol: TCP
        # mysql
        - containerPort: 3306
          hostPort: 3306
          protocol: TCP
        # haproxy stats
        - containerPort: 1936
          hostPort: 1936
          protocol: TCP
        # gogs ssh
        - containerPort: 2222
          hostPort: 2222
          protocol: TCP
        volumeMounts:
        - name: ssl-volume
          readOnly: true
          mountPath: &amp;quot;/ssl&amp;quot;
        resources: {}
        args:
        - --tcp-services=my-gogs-ssh:2222
        - --ssl-cert=/ssl/server.pem
        - --ssl-ca-cert=/ssl/ca.crt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;He&amp;rsquo;re you can see that I&amp;rsquo;ve exposed the &lt;code&gt;my-gogs-ssh&lt;/code&gt; as a TCP service.&lt;/p&gt;

&lt;p&gt;Now we&amp;rsquo;re ready to expose our Gogs service to the outside world. We need to
change our Gogs service from
&lt;a href=&#34;http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/&#34;&gt;Part 1&lt;/a&gt; slightly and add
some annotations.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  annotations:
    serviceloadbalancer/lb.sslTerm: &amp;quot;true&amp;quot;
    serviceloadbalancer/lb.host: &amp;quot;gogs.replace.me&amp;quot;
    serviceloadbalancer/lb.cookie-sticky-session: &amp;quot;true&amp;quot;
  labels:
    app: my-gogs-service
  name: my-gogs-service
  namespace: default
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 3000
  selector:
    app: my-gogs
  sessionAffinity: None
  type: ClusterIP
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;serviceloadbalancer/lb.sslTerm: &amp;quot;true&amp;quot;&lt;/code&gt; annotation says that we wan&amp;rsquo;t to
use &lt;code&gt;https&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serviceloadbalancer/lb.host: &amp;quot;gogs.replace.me&amp;quot;&lt;/code&gt; is the &lt;code&gt;virtual host&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serviceloadbalancer/lb.cookie-sticky-session: &amp;quot;true&amp;quot;&lt;/code&gt; enables sticky sessions
between your pods (&lt;code&gt;replicas &amp;gt; 1&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If everything is working you should be rewarded with a &lt;code&gt;ha-proxy&lt;/code&gt; status page
where you can monitor your exposed services. Fire up
&lt;code&gt;http://&amp;lt;loadbalancer ip&amp;gt;:1936/&lt;/code&gt; in your favorite browser and take a look.&lt;/p&gt;

&lt;p&gt;That is all for now. In the next part we will take a look at the &lt;code&gt;CI&lt;/code&gt; tool
&lt;a href=&#34;https://github.com/drone/drone&#34;&gt;Drone&lt;/a&gt; and how to get it working on our
&lt;em&gt;Kubernetes-ARM&lt;/em&gt; cluster. Prepare for a journey &lt;em&gt;down the rabbit hole&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gogs and Drone On Kubernetes-ARM - Part 1</title>
      <link>http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/</link>
      <pubDate>Sun, 07 Feb 2016 11:17:02 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/07/gogs-and-drone-on-kubernetes-arm---part-1/</guid>
      <description>&lt;p&gt;This is part 1 in a series of posts describing how I have setup Gogs and Drone
on my Kubernetes-ARM cluster. &lt;a href=&#34;https://gogs.io/&#34;&gt;Gogs - Go Git Service&lt;/a&gt; is
&lt;em&gt;A painless self-hosted Git service&lt;/em&gt; and is a great alternative when you can&amp;rsquo;t
use GitHub or wan&amp;rsquo;t to host your own Git service.&lt;/p&gt;

&lt;p&gt;The easiest way to get started with Gogs (and of course the only alternative if
you wan&amp;rsquo;t to use Kubernetes) is to use a Docker image. Gogs has a Docker image
ready on Docker Hub. Unfortunately that image won&amp;rsquo;t work on ARM.
Thankfully the Hypriot team has two Gogs Docker images ready: &lt;a href=&#34;https://hub.docker.com/r/hypriot/rpi-gogs-raspbian/&#34;&gt;hypriot/rpi-gogs-raspbian&lt;/a&gt;
and &lt;a href=&#34;https://hub.docker.com/r/hypriot/rpi-gogs-alpine/&#34;&gt;hypriot/rpi-gogs-alpine&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lets try it out:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl run my-gogs --image=hypriot/rpi-gogs-alpine --replicas=1 --port=3000
replicationcontroller &amp;quot;my-gogs-service&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we can see our pod is running:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get pods
NAME                      READY     STATUS    RESTARTS   AGE
my-gogs-3adh8             1/1       Running   0          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it&amp;rsquo;s time to expose our new pod as a service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl expose rc my-gogs --port=80 --target-port=3000 --name=my-gogs-service
service &amp;quot;my-gogs-service&amp;quot; exposed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If all went well we can now access or new service:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl -s http://[master-ip]:8080/api/v1/proxy/namespaces/default/services/my-gogs-service/install|grep Version:
&amp;lt;p class=&amp;quot;left&amp;quot; id=&amp;quot;footer-rights&amp;quot;&amp;gt;© 2015 Gogs · Version: 0.6.1.0325 Beta Page:&amp;lt;strong&amp;gt;2259ms&amp;lt;/strong&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But oops! that´s kind of an old version. Our goal is to use Gogs together with
Drone and this won&amp;rsquo;t work. We need a version greater than &lt;code&gt;0.6.16.1022&lt;/code&gt;
(&lt;a href=&#34;http://readme.drone.io/setup/gogs/&#34;&gt;see&lt;/a&gt;). I guess this is the difference
between &lt;em&gt;leading edge&lt;/em&gt; and &lt;em&gt;bleeding edge&lt;/em&gt;. Again we&amp;rsquo;re saved by some one else&amp;rsquo;s
work. Gogs has a &lt;code&gt;Dockerfile.rpi&lt;/code&gt; ready that we can use to build our own image.
I&amp;rsquo;ve built and pushed an image to Docker Hub that you can use:
&lt;a href=&#34;https://hub.docker.com/r/larmog/rpi-gogs/&#34;&gt;&lt;code&gt;larmog/rpi-gogs&lt;/code&gt;&lt;/a&gt; that is &lt;code&gt;33MB&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So lets repeat the steps above with the new image and &lt;code&gt;curl&lt;/code&gt; for the version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ curl -s http://[master-ip]:8080/api/v1/proxy/namespaces/default/services/my-gogs-service/install|grep Version
© 2016 Gogs Version: 0.8.23.0126 Page: &amp;lt;strong&amp;gt;1622ms&amp;lt;/strong&amp;gt; Template: &amp;lt;strong&amp;gt;1619ms&amp;lt;/strong&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Version &lt;code&gt;0.8.23.0126&lt;/code&gt;, that looks so much better don&amp;rsquo;t, you think?&lt;/p&gt;

&lt;p&gt;Next step is to install Gogs. But hey&amp;hellip; wait a minute - what about persistence?
We need to add a &lt;em&gt;Volume&lt;/em&gt;. I&amp;rsquo;m using my home NAS, a DiskStation, over NFS. The
only thing we need to do is to share a volume over NFS and install NFS on our
nodes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo apt-get -y install nfs-common
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we need to create a &lt;code&gt;PersistentVolume&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-gogs
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: /volume1/kbn1/gogs
    server: my-nfs-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-gogs
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and lastly mount the volume in our pod template:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ReplicationController
metadata:
  labels:
    app: my-gogs
  name: my-gogs
  namespace: default
spec:
  replicas: 1
  selector:
    app: my-gogs
  template:
    metadata:
      labels:
        app: my-gogs
    spec:
      containers:
      - image: larmog/rpi-gogs:0.8.23.0126-2
        imagePullPolicy: IfNotPresent
        name: my-gogs
        volumeMounts:
        - mountPath: &amp;quot;/data&amp;quot;
          name: persistentdata
        resources: {}
        ports:
          - containerPort: 3000
            name: web   
            protocol: TCP
          - containerPort: 22
            name: ssh
            protocol: TCP
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      volumes:
        - name: persistentdata
          persistentVolumeClaim:
            claimName: pvc-gogs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NFS v4 is kind of hard to use if you don&amp;rsquo;t have synchronized your users and
groups in your domain. I use &lt;code&gt;all_squash&lt;/code&gt; to a specific UID/GID in order to get
it to work with my NAS, and that works fine for Gogs but I&amp;rsquo;ve got plans to
replace NFS with &lt;a href=&#34;https://www.gluster.org/&#34;&gt;GlusterFS&lt;/a&gt; and it&amp;rsquo;s on the &lt;code&gt;TODO&lt;/code&gt;
list.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s enjoy the fruit of our work (or as we say in Sweden: &amp;ldquo;ett Ernst ögonblick&amp;rdquo;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ kubectl get services
NAME             CLUSTER_IP   EXTERNAL_IP   PORT(S)    SELECTOR       AGE
my-gogs-service  10.0.0.85    &amp;lt;none&amp;gt;        80/TCP     app=gogs       9d
my-gogs-ssh      10.0.0.216   nodes         2222/TCP   app=gogs       9d
kubernetes       10.0.0.1     &amp;lt;none&amp;gt;        443/TCP    &amp;lt;none&amp;gt;         25d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that I&amp;rsquo;ve also created a service for the &lt;code&gt;ssh&lt;/code&gt; port.
Now we can complete the Gogs installation. Open the url (http://[master-ip]:8080/api/v1/proxy/namespaces/default/services/my-gogs-service)
and complete the installation.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;http://larmog.github.io/2016/02/08/gogs-and-drone-on-kubernetes-arm---part-2/&#34;&gt;Part 2&lt;/a&gt; I will
explain how to set up &lt;code&gt;service-loadbalancer&lt;/code&gt; to expose your services outside your
cluster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes On ARM</title>
      <link>http://larmog.github.io/2016/02/06/kubernetes-on-arm/</link>
      <pubDate>Sat, 06 Feb 2016 10:33:08 +0100</pubDate>
      
      <guid>http://larmog.github.io/2016/02/06/kubernetes-on-arm/</guid>
      <description>&lt;p&gt;I really like Kubernetes for orchestrating Docker containers. If you&amp;rsquo;re don&amp;rsquo;t
familiar with Kubernetes I can highly recommend to take a look at
&lt;a href=&#34;http://kubernetes.io&#34;&gt;kubernetes.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can easily run you&amp;rsquo;re own Kubernetes cluster on your local machine using
Vagrant or run Kubernetes in the cloud using AWS, Azure or Google Compute.
But I like a more hands on solution and I always wanted my own &amp;ldquo;data center&amp;rdquo;.
On the other hand I don&amp;rsquo;t want to spend a fortune building a DC just for fun.&lt;/p&gt;

&lt;p&gt;A great alternative is to use ARM SoC boards like Raspberry PI. Thank&amp;rsquo;s to
&lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas Käldström&lt;/a&gt; and
&lt;a href=&#34;http://blog.hypriot.com/&#34;&gt;Hypriot&lt;/a&gt; this is a rather straight forward process.&lt;/p&gt;

&lt;p&gt;I used the &lt;code&gt;hypriotos&lt;/code&gt; image and Lucas &lt;code&gt;deb&lt;/code&gt;-package to install &lt;code&gt;kube-config&lt;/code&gt;.&lt;/p&gt;


&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Six Raspberry Pi:s in a cluster&lt;/span&gt;
          
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/IMG_1936.png&#34;  /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;



&lt;p&gt;
&lt;figure&gt;
  &lt;div class=&#34;card blue-grey teal lighten-5&#34;&gt;
    &lt;div class=&#34;card-content black-text&#34;&gt;
      
      &lt;figcaption&gt;
          &lt;span class=&#34;card-title black-text&#34;&gt;Nodes in my cluster&lt;/span&gt;
          &lt;p&gt;
          Here you can see the nodes in the cluster. I&amp;#39;m using the service-loadbalancer addon (a topic for another post).
          
              
          
        &lt;/p&gt; 
      &lt;/figcaption&gt;
      
      
          &lt;img class=&#34;responsive-img&#34; src=&#34;http://larmog.github.io/media/k8s.png&#34; alt=&#34;Here you can see the nodes in the cluster. I&amp;#39;m using the service-loadbalancer addon (a topic for another post).&#34; /&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;

&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>